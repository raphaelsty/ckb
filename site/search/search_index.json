{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Contextual Knowledge Bases CKB is an informal implementation of the model focusing on the link prediction task Inductive Entity Representations from Text via Link Prediction . This tool allows to train transformers i.e. Bert (and all his friends) to build embeddings of the entities of a knowledge graph. The CKB library is dedicated to knowledge bases and allows to fine-tune HuggingFace models using the link prediction task. The objective of this fine-tuning task is to make accurate embeddings of the knowledge graph entities. The link prediction task aims at training a model to find the missing element of an RDF triplet. For the triplet (France, is_part_of, ?) , the model should retrieve the entity Europe . After fine-tuning the transformer on the link prediction task, it can be used to build an entity search engine. It can perform tasks related to the completion of knowledge graphs. Finally, it can be used for any downstream task such as classification. The original paper replaces the embeddings traditionally used with models dedicated to knowledge graphs with an encoder (TransE vs BERT). Here, the encoder is a pre-trained transformer. The use of a transformer has many advantages such as the construction of contextualized latent representations of entities. In addition, this model can encode entities that it has never seen with the textual description of the entity. The learning time is much longer than a classical TransE model, but the model converges with fewer epochs. Documentation \u00b6 Installation \u00b6 pip install git+https://github.com/raphaelsty/ckb Train your own model: \u00b6 from ckb import compose from ckb import datasets from ckb import evaluation from ckb import losses from ckb import models from ckb import sampling from ckb import scoring from transformers import BertTokenizer from transformers import BertModel import torch _ = torch . manual_seed ( 42 ) device = 'cuda' # You should own a GPU, it is very slow with cpu. # Train, valid and test sets are a list of triples. train = [ ( 'My Favorite Carrot Cake Recipe' , 'made_with' , 'Brown Sugar' ), ( 'My Favorite Carrot Cake Recipe' , 'made_with' , 'Oil' ), ( 'My Favorite Carrot Cake Recipe' , 'made_with' , 'Applesauce' ), ( 'Classic Cheesecake Recipe' , 'made_with' , 'Block cream cheese' ), ( 'Classic Cheesecake Recipe' , 'made_with' , 'Sugar' ), ( 'Classic Cheesecake Recipe' , 'made_with' , 'Sour cream' ), ] valid = [ ( 'My Favorite Carrot Cake Recipe' , 'made_with' , 'A bit of sugar' ), ( 'Classic Cheesecake Recipe' , 'made_with' , 'Eggs' ) ] test = [ ( 'My Favorite Strawberry Cake Recipe' , 'made_with' , 'Fresh Strawberry' ) ] # Initialize the dataset, batch size should be small to avoid RAM exceed. dataset = datasets . Dataset ( batch_size = 1 , train = train , valid = valid , test = test , seed = 42 , ) model = models . Transformer ( model = BertModel . from_pretrained ( 'bert-base-uncased' ), tokenizer = BertTokenizer . from_pretrained ( 'bert-base-uncased' ), entities = dataset . entities , relations = dataset . relations , gamma = 9 , scoring = scoring . TransE (), device = device , ) model = model . to ( device ) optimizer = torch . optim . Adam ( filter ( lambda p : p . requires_grad , model . parameters ()), lr = 0.00005 , ) evaluation = evaluation . Evaluation ( entities = dataset . entities , relations = dataset . relations , true_triples = dataset . train + dataset . valid + dataset . test , batch_size = 1 , device = device , ) # Number of negative samples to show to the model for each batch. # Should be small to avoid memory error. sampling = sampling . NegativeSampling ( size = 1 , entities = dataset . entities , relations = dataset . relations , train_triples = dataset . train , ) pipeline = compose . Pipeline ( epochs = 20 , eval_every = 3 , # Eval the model every {eval_every} epochs. early_stopping_rounds = 1 , device = device , ) pipeline = pipeline . learn ( model = model , dataset = dataset , evaluation = evaluation , sampling = sampling , optimizer = optimizer , loss = losses . Adversarial ( alpha = 0.5 ), ) Encode entities: \u00b6 embeddings = {} for _ , e in model . entities . items (): with torch . no_grad (): embeddings [ e ] = model . encoder ([ e ]) . cpu () Encode new entities: \u00b6 new_entities = [ 'My favourite apple pie' , 'How to make croissant' , 'Pain au chocolat with coffee' , ] embeddings = {} for e in new_entities : with torch . no_grad (): embeddings [ e ] = model . encoder ([ e ]) . cpu () Save trained model: \u00b6 torch . save ( model , 'model_ckb.pth' ) Load saved model: \u00b6 model = torch . load ( f 'model_ckb.pth' ) device = 'cuda' model . device = device model . to ( device ) Official repository \ud83d\udc4d \u00b6 The official repository is available at dfdazac/blp .","title":"Home"},{"location":"#installation","text":"pip install git+https://github.com/raphaelsty/ckb","title":"Installation"},{"location":"#train-your-own-model","text":"from ckb import compose from ckb import datasets from ckb import evaluation from ckb import losses from ckb import models from ckb import sampling from ckb import scoring from transformers import BertTokenizer from transformers import BertModel import torch _ = torch . manual_seed ( 42 ) device = 'cuda' # You should own a GPU, it is very slow with cpu. # Train, valid and test sets are a list of triples. train = [ ( 'My Favorite Carrot Cake Recipe' , 'made_with' , 'Brown Sugar' ), ( 'My Favorite Carrot Cake Recipe' , 'made_with' , 'Oil' ), ( 'My Favorite Carrot Cake Recipe' , 'made_with' , 'Applesauce' ), ( 'Classic Cheesecake Recipe' , 'made_with' , 'Block cream cheese' ), ( 'Classic Cheesecake Recipe' , 'made_with' , 'Sugar' ), ( 'Classic Cheesecake Recipe' , 'made_with' , 'Sour cream' ), ] valid = [ ( 'My Favorite Carrot Cake Recipe' , 'made_with' , 'A bit of sugar' ), ( 'Classic Cheesecake Recipe' , 'made_with' , 'Eggs' ) ] test = [ ( 'My Favorite Strawberry Cake Recipe' , 'made_with' , 'Fresh Strawberry' ) ] # Initialize the dataset, batch size should be small to avoid RAM exceed. dataset = datasets . Dataset ( batch_size = 1 , train = train , valid = valid , test = test , seed = 42 , ) model = models . Transformer ( model = BertModel . from_pretrained ( 'bert-base-uncased' ), tokenizer = BertTokenizer . from_pretrained ( 'bert-base-uncased' ), entities = dataset . entities , relations = dataset . relations , gamma = 9 , scoring = scoring . TransE (), device = device , ) model = model . to ( device ) optimizer = torch . optim . Adam ( filter ( lambda p : p . requires_grad , model . parameters ()), lr = 0.00005 , ) evaluation = evaluation . Evaluation ( entities = dataset . entities , relations = dataset . relations , true_triples = dataset . train + dataset . valid + dataset . test , batch_size = 1 , device = device , ) # Number of negative samples to show to the model for each batch. # Should be small to avoid memory error. sampling = sampling . NegativeSampling ( size = 1 , entities = dataset . entities , relations = dataset . relations , train_triples = dataset . train , ) pipeline = compose . Pipeline ( epochs = 20 , eval_every = 3 , # Eval the model every {eval_every} epochs. early_stopping_rounds = 1 , device = device , ) pipeline = pipeline . learn ( model = model , dataset = dataset , evaluation = evaluation , sampling = sampling , optimizer = optimizer , loss = losses . Adversarial ( alpha = 0.5 ), )","title":"Train your own model:"},{"location":"#encode-entities","text":"embeddings = {} for _ , e in model . entities . items (): with torch . no_grad (): embeddings [ e ] = model . encoder ([ e ]) . cpu ()","title":"Encode entities:"},{"location":"#encode-new-entities","text":"new_entities = [ 'My favourite apple pie' , 'How to make croissant' , 'Pain au chocolat with coffee' , ] embeddings = {} for e in new_entities : with torch . no_grad (): embeddings [ e ] = model . encoder ([ e ]) . cpu ()","title":"Encode new entities:"},{"location":"#save-trained-model","text":"torch . save ( model , 'model_ckb.pth' )","title":"Save trained model:"},{"location":"#load-saved-model","text":"model = torch . load ( f 'model_ckb.pth' ) device = 'cuda' model . device = device model . to ( device )","title":"Load saved model:"},{"location":"#official-repository","text":"The official repository is available at dfdazac/blp .","title":"Official repository \ud83d\udc4d"},{"location":"api/overview/","text":"Overview \u00b6 compose \u00b6 Pipeline datasets \u00b6 Dataset Fb15k237 Semanlink Wn18rr evaluation \u00b6 Evaluation models \u00b6 BaseModel DistillBert FlauBERT Transformer sampling \u00b6 NegativeSampling scoring \u00b6 ComplEx DistMult RotatE Scoring TransE pRotatE utils \u00b6 read_csv","title":"Overview"},{"location":"api/overview/#overview","text":"","title":"Overview"},{"location":"api/overview/#compose","text":"Pipeline","title":"compose"},{"location":"api/overview/#datasets","text":"Dataset Fb15k237 Semanlink Wn18rr","title":"datasets"},{"location":"api/overview/#evaluation","text":"Evaluation","title":"evaluation"},{"location":"api/overview/#models","text":"BaseModel DistillBert FlauBERT Transformer","title":"models"},{"location":"api/overview/#sampling","text":"NegativeSampling","title":"sampling"},{"location":"api/overview/#scoring","text":"ComplEx DistMult RotatE Scoring TransE pRotatE","title":"scoring"},{"location":"api/overview/#utils","text":"read_csv","title":"utils"},{"location":"api/compose/Pipeline/","text":"Pipeline \u00b6 Pipeline dedicated to automate training model. Parameters \u00b6 epochs eval_every \u2013 defaults to 1 early_stopping_rounds \u2013 defaults to 3 device \u2013 defaults to cuda Examples \u00b6 >>> from ckb import compose >>> from ckb import datasets >>> from ckb import evaluation >>> from ckb import losses >>> from ckb import models >>> from ckb import sampling >>> from ckb import scoring >>> import torch >>> _ = torch . manual_seed ( 42 ) >>> device = 'cpu' >>> train = [( 'mkb' , 'is_a' , 'library' )] >>> valid = [( 'ckb' , 'is_a' , 'library' ), ( 'github' , 'is_a' , 'tool' )] >>> test = [( 'mkb' , 'is_a' , 'tool' ), ( 'ckb' , 'is_a' , 'tool' )] >>> dataset = datasets . Dataset ( ... batch_size = 1 , ... train = train , ... valid = valid , ... test = test , ... seed = 42 , ... ) >>> model = models . DistillBert ( ... entities = dataset . entities , ... relations = dataset . relations , ... gamma = 9 , ... scoring = scoring . TransE (), ... device = device , ... ) >>> model = model . to ( device ) >>> optimizer = torch . optim . Adam ( ... filter ( lambda p : p . requires_grad , model . parameters ()), ... lr = 0.00005 , ... ) >>> evaluation = evaluation . Evaluation ( ... entities = dataset . entities , ... relations = dataset . relations , ... true_triples = dataset . train + dataset . valid + dataset . test , ... batch_size = 1 , ... device = device , ... ) >>> sampling = sampling . NegativeSampling ( ... size = 1 , ... entities = dataset . entities , ... relations = dataset . relations , ... train_triples = dataset . train , ... ) >>> pipeline = compose . Pipeline ( ... epochs = 1 , ... eval_every = 1 , ... early_stopping_rounds = 1 , ... device = device , ... ) >>> pipeline = pipeline . learn ( ... model = model , ... dataset = dataset , ... evaluation = evaluation , ... sampling = sampling , ... optimizer = optimizer , ... loss = losses . Adversarial ( alpha = 0.5 ), ... ) < BLANKLINE > Epoch : 0. Validation : MRR : 0.375 MR : 2.75 HITS @ 1 : 0.0 HITS @ 3 : 1.0 HITS @ 10 : 1.0 MRR_relations : 1.0 MR_relations : 1.0 HITS @ 1 _relations : 1.0 HITS @ 3 _relations : 1.0 HITS @ 10 _relations : 1.0 Test : MRR : 0.375 MR : 2.75 HITS @ 1 : 0.0 HITS @ 3 : 1.0 HITS @ 10 : 1.0 MRR_relations : 1.0 MR_relations : 1.0 HITS @ 1 _relations : 1.0 HITS @ 3 _relations : 1.0 HITS @ 10 _relations : 1.0 < BLANKLINE > Epoch : 0. < BLANKLINE > Validation : MRR : 0.375 MR : 2.75 HITS @ 1 : 0.0 HITS @ 3 : 1.0 HITS @ 10 : 1.0 MRR_relations : 1.0 MR_relations : 1.0 HITS @ 1 _relations : 1.0 HITS @ 3 _relations : 1.0 HITS @ 10 _relations : 1.0 Test : MRR : 0.375 MR : 2.75 HITS @ 1 : 0.0 HITS @ 3 : 1.0 HITS @ 10 : 1.0 MRR_relations : 1.0 MR_relations : 1.0 HITS @ 1 _relations : 1.0 HITS @ 3 _relations : 1.0 HITS @ 10 _relations : 1.0 Methods \u00b6 learn print_metrics","title":"Pipeline"},{"location":"api/compose/Pipeline/#pipeline","text":"Pipeline dedicated to automate training model.","title":"Pipeline"},{"location":"api/compose/Pipeline/#parameters","text":"epochs eval_every \u2013 defaults to 1 early_stopping_rounds \u2013 defaults to 3 device \u2013 defaults to cuda","title":"Parameters"},{"location":"api/compose/Pipeline/#examples","text":">>> from ckb import compose >>> from ckb import datasets >>> from ckb import evaluation >>> from ckb import losses >>> from ckb import models >>> from ckb import sampling >>> from ckb import scoring >>> import torch >>> _ = torch . manual_seed ( 42 ) >>> device = 'cpu' >>> train = [( 'mkb' , 'is_a' , 'library' )] >>> valid = [( 'ckb' , 'is_a' , 'library' ), ( 'github' , 'is_a' , 'tool' )] >>> test = [( 'mkb' , 'is_a' , 'tool' ), ( 'ckb' , 'is_a' , 'tool' )] >>> dataset = datasets . Dataset ( ... batch_size = 1 , ... train = train , ... valid = valid , ... test = test , ... seed = 42 , ... ) >>> model = models . DistillBert ( ... entities = dataset . entities , ... relations = dataset . relations , ... gamma = 9 , ... scoring = scoring . TransE (), ... device = device , ... ) >>> model = model . to ( device ) >>> optimizer = torch . optim . Adam ( ... filter ( lambda p : p . requires_grad , model . parameters ()), ... lr = 0.00005 , ... ) >>> evaluation = evaluation . Evaluation ( ... entities = dataset . entities , ... relations = dataset . relations , ... true_triples = dataset . train + dataset . valid + dataset . test , ... batch_size = 1 , ... device = device , ... ) >>> sampling = sampling . NegativeSampling ( ... size = 1 , ... entities = dataset . entities , ... relations = dataset . relations , ... train_triples = dataset . train , ... ) >>> pipeline = compose . Pipeline ( ... epochs = 1 , ... eval_every = 1 , ... early_stopping_rounds = 1 , ... device = device , ... ) >>> pipeline = pipeline . learn ( ... model = model , ... dataset = dataset , ... evaluation = evaluation , ... sampling = sampling , ... optimizer = optimizer , ... loss = losses . Adversarial ( alpha = 0.5 ), ... ) < BLANKLINE > Epoch : 0. Validation : MRR : 0.375 MR : 2.75 HITS @ 1 : 0.0 HITS @ 3 : 1.0 HITS @ 10 : 1.0 MRR_relations : 1.0 MR_relations : 1.0 HITS @ 1 _relations : 1.0 HITS @ 3 _relations : 1.0 HITS @ 10 _relations : 1.0 Test : MRR : 0.375 MR : 2.75 HITS @ 1 : 0.0 HITS @ 3 : 1.0 HITS @ 10 : 1.0 MRR_relations : 1.0 MR_relations : 1.0 HITS @ 1 _relations : 1.0 HITS @ 3 _relations : 1.0 HITS @ 10 _relations : 1.0 < BLANKLINE > Epoch : 0. < BLANKLINE > Validation : MRR : 0.375 MR : 2.75 HITS @ 1 : 0.0 HITS @ 3 : 1.0 HITS @ 10 : 1.0 MRR_relations : 1.0 MR_relations : 1.0 HITS @ 1 _relations : 1.0 HITS @ 3 _relations : 1.0 HITS @ 10 _relations : 1.0 Test : MRR : 0.375 MR : 2.75 HITS @ 1 : 0.0 HITS @ 3 : 1.0 HITS @ 10 : 1.0 MRR_relations : 1.0 MR_relations : 1.0 HITS @ 1 _relations : 1.0 HITS @ 3 _relations : 1.0 HITS @ 10 _relations : 1.0","title":"Examples"},{"location":"api/compose/Pipeline/#methods","text":"learn print_metrics","title":"Methods"},{"location":"api/datasets/Dataset/","text":"Dataset \u00b6 Custom dataset creation The Dataset class allows to iterate on the data of a dataset. Dataset takes entities as input, relations, training data and optional validation and test data. Training data, validation and testing must be organized in the form of a triplet list. Entities and relations must be in a dictionary where the key is the label of the entity or relationship and the value must be the index of the entity / relation. Parameters \u00b6 train batch_size entities \u2013 defaults to None relations \u2013 defaults to None valid \u2013 defaults to None test \u2013 defaults to None shuffle \u2013 defaults to True pre_compute \u2013 defaults to True num_workers \u2013 defaults to 1 seed \u2013 defaults to None Attributes \u00b6 n_entity (int): Number of entities. n_relation (int): Number of relations. Examples \u00b6 >>> from ckb import datasets >>> train = [ ... ( '\ud83d\udc1d' , 'is' , 'animal' ), ... ( '\ud83d\udc3b' , 'is' , 'animal' ), ... ( '\ud83d\udc0d' , 'is' , 'animal' ), ... ( '\ud83e\udd94' , 'is' , 'animal' ), ... ( '\ud83e\udd93' , 'is' , 'animal' ), ... ( '\ud83e\udd92' , 'is' , 'animal' ), ... ( '\ud83e\udd98' , 'is' , 'animal' ), ... ( '\ud83e\udd9d' , 'is' , 'animal' ), ... ( '\ud83e\udd9e' , 'is' , 'animal' ), ... ( '\ud83e\udda2' , 'is' , 'animal' ), ... ] >>> test = [ ... ( '\ud83d\udc1d' , 'is' , 'animal' ), ... ( '\ud83d\udc3b' , 'is' , 'animal' ), ... ( '\ud83d\udc0d' , 'is' , 'animal' ), ... ( '\ud83e\udd94' , 'is' , 'animal' ), ... ( '\ud83e\udd93' , 'is' , 'animal' ), ... ( '\ud83e\udd92' , 'is' , 'animal' ), ... ( '\ud83e\udd98' , 'is' , 'animal' ), ... ( '\ud83e\udd9d' , 'is' , 'animal' ), ... ( '\ud83e\udd9e' , 'is' , 'animal' ), ... ( '\ud83e\udda2' , 'is' , 'animal' ), ... ] >>> dataset = datasets . Dataset ( train = train , test = test , batch_size = 2 , seed = 42 , shuffle = False ) >>> dataset Dataset dataset Batch size 2 Entities 11 Relations 1 Shuffle False Train triples 10 Validation triples 0 Test triples 10 >>> dataset . entities { '\ud83d\udc1d' : 0 , '\ud83d\udc3b' : 1 , '\ud83d\udc0d' : 2 , '\ud83e\udd94' : 3 , '\ud83e\udd93' : 4 , '\ud83e\udd92' : 5 , '\ud83e\udd98' : 6 , '\ud83e\udd9d' : 7 , '\ud83e\udd9e' : 8 , '\ud83e\udda2' : 9 , 'animal' : 10 } Methods \u00b6 fetch get_train_loader Initialize train dataset loader. Parameters mode mapping_entities Construct mapping entities. mapping_relations Construct mapping relations. test_dataset test_stream validation_dataset References \u00b6 Sun, Zhiqing, et al. \"Rotate: Knowledge graph embedding by relational rotation in complex space.\" arXiv preprint arXiv:1902.10197 (2019). \u21a9 Knowledge Graph Embedding \u21a9","title":"Dataset"},{"location":"api/datasets/Dataset/#dataset","text":"Custom dataset creation The Dataset class allows to iterate on the data of a dataset. Dataset takes entities as input, relations, training data and optional validation and test data. Training data, validation and testing must be organized in the form of a triplet list. Entities and relations must be in a dictionary where the key is the label of the entity or relationship and the value must be the index of the entity / relation.","title":"Dataset"},{"location":"api/datasets/Dataset/#parameters","text":"train batch_size entities \u2013 defaults to None relations \u2013 defaults to None valid \u2013 defaults to None test \u2013 defaults to None shuffle \u2013 defaults to True pre_compute \u2013 defaults to True num_workers \u2013 defaults to 1 seed \u2013 defaults to None","title":"Parameters"},{"location":"api/datasets/Dataset/#attributes","text":"n_entity (int): Number of entities. n_relation (int): Number of relations.","title":"Attributes"},{"location":"api/datasets/Dataset/#examples","text":">>> from ckb import datasets >>> train = [ ... ( '\ud83d\udc1d' , 'is' , 'animal' ), ... ( '\ud83d\udc3b' , 'is' , 'animal' ), ... ( '\ud83d\udc0d' , 'is' , 'animal' ), ... ( '\ud83e\udd94' , 'is' , 'animal' ), ... ( '\ud83e\udd93' , 'is' , 'animal' ), ... ( '\ud83e\udd92' , 'is' , 'animal' ), ... ( '\ud83e\udd98' , 'is' , 'animal' ), ... ( '\ud83e\udd9d' , 'is' , 'animal' ), ... ( '\ud83e\udd9e' , 'is' , 'animal' ), ... ( '\ud83e\udda2' , 'is' , 'animal' ), ... ] >>> test = [ ... ( '\ud83d\udc1d' , 'is' , 'animal' ), ... ( '\ud83d\udc3b' , 'is' , 'animal' ), ... ( '\ud83d\udc0d' , 'is' , 'animal' ), ... ( '\ud83e\udd94' , 'is' , 'animal' ), ... ( '\ud83e\udd93' , 'is' , 'animal' ), ... ( '\ud83e\udd92' , 'is' , 'animal' ), ... ( '\ud83e\udd98' , 'is' , 'animal' ), ... ( '\ud83e\udd9d' , 'is' , 'animal' ), ... ( '\ud83e\udd9e' , 'is' , 'animal' ), ... ( '\ud83e\udda2' , 'is' , 'animal' ), ... ] >>> dataset = datasets . Dataset ( train = train , test = test , batch_size = 2 , seed = 42 , shuffle = False ) >>> dataset Dataset dataset Batch size 2 Entities 11 Relations 1 Shuffle False Train triples 10 Validation triples 0 Test triples 10 >>> dataset . entities { '\ud83d\udc1d' : 0 , '\ud83d\udc3b' : 1 , '\ud83d\udc0d' : 2 , '\ud83e\udd94' : 3 , '\ud83e\udd93' : 4 , '\ud83e\udd92' : 5 , '\ud83e\udd98' : 6 , '\ud83e\udd9d' : 7 , '\ud83e\udd9e' : 8 , '\ud83e\udda2' : 9 , 'animal' : 10 }","title":"Examples"},{"location":"api/datasets/Dataset/#methods","text":"fetch get_train_loader Initialize train dataset loader. Parameters mode mapping_entities Construct mapping entities. mapping_relations Construct mapping relations. test_dataset test_stream validation_dataset","title":"Methods"},{"location":"api/datasets/Dataset/#references","text":"Sun, Zhiqing, et al. \"Rotate: Knowledge graph embedding by relational rotation in complex space.\" arXiv preprint arXiv:1902.10197 (2019). \u21a9 Knowledge Graph Embedding \u21a9","title":"References"},{"location":"api/datasets/Fb15k237/","text":"Fb15k237 \u00b6 Wn18rr dataset. Parameters \u00b6 batch_size shuffle \u2013 defaults to True pre_compute \u2013 defaults to True num_workers \u2013 defaults to 1 seed \u2013 defaults to None Attributes \u00b6 name train_triples true_triples Get all true triples from the dataset. Examples \u00b6 >>> from ckb import datasets >>> dataset = datasets . Fb15k237 ( batch_size = 1 , pre_compute = True , shuffle = True , seed = 42 ) >>> dataset Fb15k237 dataset Batch size 1 Entities 14265 Relations 237 Shuffle True Train triples 271364 Validation triples 17528 Test triples 20460 Methods \u00b6 fetch get_train_loader Initialize train dataset loader. Parameters mode mapping_entities Construct mapping entities. mapping_relations Construct mapping relations. test_dataset test_stream validation_dataset References \u00b6 Liang Yao, Chengsheng Mao, and Yuan Luo. 2019. Kg- bert: Bert for knowledge graph completion. arXiv preprint arXiv:1909.03193. \u21a9","title":"Fb15k237"},{"location":"api/datasets/Fb15k237/#fb15k237","text":"Wn18rr dataset.","title":"Fb15k237"},{"location":"api/datasets/Fb15k237/#parameters","text":"batch_size shuffle \u2013 defaults to True pre_compute \u2013 defaults to True num_workers \u2013 defaults to 1 seed \u2013 defaults to None","title":"Parameters"},{"location":"api/datasets/Fb15k237/#attributes","text":"name train_triples true_triples Get all true triples from the dataset.","title":"Attributes"},{"location":"api/datasets/Fb15k237/#examples","text":">>> from ckb import datasets >>> dataset = datasets . Fb15k237 ( batch_size = 1 , pre_compute = True , shuffle = True , seed = 42 ) >>> dataset Fb15k237 dataset Batch size 1 Entities 14265 Relations 237 Shuffle True Train triples 271364 Validation triples 17528 Test triples 20460","title":"Examples"},{"location":"api/datasets/Fb15k237/#methods","text":"fetch get_train_loader Initialize train dataset loader. Parameters mode mapping_entities Construct mapping entities. mapping_relations Construct mapping relations. test_dataset test_stream validation_dataset","title":"Methods"},{"location":"api/datasets/Fb15k237/#references","text":"Liang Yao, Chengsheng Mao, and Yuan Luo. 2019. Kg- bert: Bert for knowledge graph completion. arXiv preprint arXiv:1909.03193. \u21a9","title":"References"},{"location":"api/datasets/Semanlink/","text":"Semanlink \u00b6 Semanlink dataset. Parameters \u00b6 batch_size shuffle \u2013 defaults to True pre_compute \u2013 defaults to True num_workers \u2013 defaults to 1 seed \u2013 defaults to None Attributes \u00b6 train (list): Training set. valid (list): Validation set. test (list): Testing set. entities (dict): Index of entities. relations (dict): Index of relations. n_entity (int): Number of entities. n_relation (int): Number of relations. Examples \u00b6 >>> from ckb import datasets >>> dataset = datasets . Semanlink ( batch_size = 1 , pre_compute = True , shuffle = True , seed = 42 ) >>> dataset Semanlink dataset Batch size 1 Entities 5454 Relations 4 Shuffle True Train triples 6422 Validation triples 803 Test triples 803 Methods \u00b6 fetch get_train_loader Initialize train dataset loader. Parameters mode mapping_entities Construct mapping entities. mapping_relations Construct mapping relations. test_dataset test_stream validation_dataset","title":"Semanlink"},{"location":"api/datasets/Semanlink/#semanlink","text":"Semanlink dataset.","title":"Semanlink"},{"location":"api/datasets/Semanlink/#parameters","text":"batch_size shuffle \u2013 defaults to True pre_compute \u2013 defaults to True num_workers \u2013 defaults to 1 seed \u2013 defaults to None","title":"Parameters"},{"location":"api/datasets/Semanlink/#attributes","text":"train (list): Training set. valid (list): Validation set. test (list): Testing set. entities (dict): Index of entities. relations (dict): Index of relations. n_entity (int): Number of entities. n_relation (int): Number of relations.","title":"Attributes"},{"location":"api/datasets/Semanlink/#examples","text":">>> from ckb import datasets >>> dataset = datasets . Semanlink ( batch_size = 1 , pre_compute = True , shuffle = True , seed = 42 ) >>> dataset Semanlink dataset Batch size 1 Entities 5454 Relations 4 Shuffle True Train triples 6422 Validation triples 803 Test triples 803","title":"Examples"},{"location":"api/datasets/Semanlink/#methods","text":"fetch get_train_loader Initialize train dataset loader. Parameters mode mapping_entities Construct mapping entities. mapping_relations Construct mapping relations. test_dataset test_stream validation_dataset","title":"Methods"},{"location":"api/datasets/Wn18rr/","text":"Wn18rr \u00b6 Wn18rr dataset. Parameters \u00b6 batch_size shuffle \u2013 defaults to True pre_compute \u2013 defaults to True num_workers \u2013 defaults to 1 seed \u2013 defaults to None Attributes \u00b6 train (list): Training set. valid (list): Validation set. test (list): Testing set. entities (dict): Index of entities. relations (dict): Index of relations. n_entity (int): Number of entities. n_relation (int): Number of relations. Examples \u00b6 >>> from ckb import datasets >>> dataset = datasets . Wn18rr ( batch_size = 1 , pre_compute = True , shuffle = True , seed = 42 ) >>> dataset Wn18rr dataset Batch size 1 Entities 40943 Relations 11 Shuffle True Train triples 86835 Validation triples 3034 Test triples 3134 Methods \u00b6 fetch get_train_loader Initialize train dataset loader. Parameters mode mapping_entities Construct mapping entities. mapping_relations Construct mapping relations. test_dataset test_stream validation_dataset References \u00b6 Liang Yao, Chengsheng Mao, and Yuan Luo. 2019. Kg- bert: Bert for knowledge graph completion. arXiv preprint arXiv:1909.03193. \u21a9","title":"Wn18rr"},{"location":"api/datasets/Wn18rr/#wn18rr","text":"Wn18rr dataset.","title":"Wn18rr"},{"location":"api/datasets/Wn18rr/#parameters","text":"batch_size shuffle \u2013 defaults to True pre_compute \u2013 defaults to True num_workers \u2013 defaults to 1 seed \u2013 defaults to None","title":"Parameters"},{"location":"api/datasets/Wn18rr/#attributes","text":"train (list): Training set. valid (list): Validation set. test (list): Testing set. entities (dict): Index of entities. relations (dict): Index of relations. n_entity (int): Number of entities. n_relation (int): Number of relations.","title":"Attributes"},{"location":"api/datasets/Wn18rr/#examples","text":">>> from ckb import datasets >>> dataset = datasets . Wn18rr ( batch_size = 1 , pre_compute = True , shuffle = True , seed = 42 ) >>> dataset Wn18rr dataset Batch size 1 Entities 40943 Relations 11 Shuffle True Train triples 86835 Validation triples 3034 Test triples 3134","title":"Examples"},{"location":"api/datasets/Wn18rr/#methods","text":"fetch get_train_loader Initialize train dataset loader. Parameters mode mapping_entities Construct mapping entities. mapping_relations Construct mapping relations. test_dataset test_stream validation_dataset","title":"Methods"},{"location":"api/datasets/Wn18rr/#references","text":"Liang Yao, Chengsheng Mao, and Yuan Luo. 2019. Kg- bert: Bert for knowledge graph completion. arXiv preprint arXiv:1909.03193. \u21a9","title":"References"},{"location":"api/evaluation/Evaluation/","text":"Evaluation \u00b6 Wrapper for MKB evaluation module. Parameters \u00b6 entities relations batch_size true_triples \u2013 defaults to [] device \u2013 defaults to cuda num_workers \u2013 defaults to 1 entities_to_drop \u2013 defaults to [] same_entities \u2013 defaults to {} Examples \u00b6 >>> from mkb import datasets >>> from ckb import evaluation >>> from ckb import models >>> from ckb import scoring >>> import torch >>> _ = torch . manual_seed ( 42 ) >>> train = [( 'mkb' , 'is_a' , 'library' ), ( 'github' , 'is_a' , 'tool' )] >>> valid = [( 'ckb' , 'is_a' , 'library' ), ( 'github' , 'is_a' , 'tool' )] >>> test = [( 'mkb' , 'is_a' , 'tool' ), ( 'ckb' , 'is_a' , 'tool' )] >>> dataset = datasets . Dataset ( ... batch_size = 1 , ... train = train , ... valid = valid , ... test = test , ... seed = 42 , ... ) >>> dataset Dataset dataset Batch size 1 Entities 5 Relations 1 Shuffle True Train triples 2 Validation triples 2 Test triples 2 >>> model = models . DistillBert ( ... entities = dataset . entities , ... relations = dataset . relations , ... gamma = 9 , ... scoring = scoring . TransE (), ... device = 'cpu' , ... ) >>> model . entities { 0 : 'mkb' , 1 : 'github' , 2 : 'ckb' , 3 : 'library' , 4 : 'tool' } >>> model DistillBert model Entities embeddings dim 768 Relations embeddings dim 768 Gamma 9.0 Number of entities 5 Number of relations 1 >>> validation = evaluation . Evaluation ( ... entities = dataset . entities , ... relations = dataset . relations , ... true_triples = dataset . train + dataset . valid + dataset . test , ... batch_size = 1 , ... device = 'cpu' , ... ) >>> validation . eval ( model = model , dataset = dataset . valid ) { 'MRR' : 0.3958 , 'MR' : 2.75 , 'HITS@1' : 0.0 , 'HITS@3' : 0.75 , 'HITS@10' : 1.0 } Methods \u00b6 compute_detailled_score compute_score detail_eval Divide input dataset relations into different categories (i.e. ONE-TO-ONE, ONE-TO-MANY, MANY-TO-ONE and MANY-TO-MANY) according to the mapping properties of relationships. Reference: 1. Bordes, Antoine, et al. \"Translating embeddings for modeling multi-relational data.\" Advances in neural information processing systems. 2013. Parameters model dataset threshold \u2013 defaults to 1.5 eval Evaluate selected model with the metrics: MRR, MR, HITS@1, HITS@3, HITS@10 Parameters model dataset eval_relations Evaluate selected model with the metrics: MRR, MR, HITS@1, HITS@3, HITS@10 Parameters model dataset get_entity_stream Get stream dedicated to link prediction. Parameters dataset get_relation_stream Get stream dedicated to relation prediction. Parameters dataset initialize Initialize model for evaluation Parameters model solve_same_entities Replace artificial entities by the target. Some description may be dedicated to the same entities. Parameters argsort types_relations Divide input dataset relations into different categories (i.e. ONE-TO-ONE, ONE-TO-MANY, MANY-TO-ONE and MANY-TO-MANY) according to the mapping properties of relationships. Parameters model dataset threshold \u2013 defaults to 1.5","title":"Evaluation"},{"location":"api/evaluation/Evaluation/#evaluation","text":"Wrapper for MKB evaluation module.","title":"Evaluation"},{"location":"api/evaluation/Evaluation/#parameters","text":"entities relations batch_size true_triples \u2013 defaults to [] device \u2013 defaults to cuda num_workers \u2013 defaults to 1 entities_to_drop \u2013 defaults to [] same_entities \u2013 defaults to {}","title":"Parameters"},{"location":"api/evaluation/Evaluation/#examples","text":">>> from mkb import datasets >>> from ckb import evaluation >>> from ckb import models >>> from ckb import scoring >>> import torch >>> _ = torch . manual_seed ( 42 ) >>> train = [( 'mkb' , 'is_a' , 'library' ), ( 'github' , 'is_a' , 'tool' )] >>> valid = [( 'ckb' , 'is_a' , 'library' ), ( 'github' , 'is_a' , 'tool' )] >>> test = [( 'mkb' , 'is_a' , 'tool' ), ( 'ckb' , 'is_a' , 'tool' )] >>> dataset = datasets . Dataset ( ... batch_size = 1 , ... train = train , ... valid = valid , ... test = test , ... seed = 42 , ... ) >>> dataset Dataset dataset Batch size 1 Entities 5 Relations 1 Shuffle True Train triples 2 Validation triples 2 Test triples 2 >>> model = models . DistillBert ( ... entities = dataset . entities , ... relations = dataset . relations , ... gamma = 9 , ... scoring = scoring . TransE (), ... device = 'cpu' , ... ) >>> model . entities { 0 : 'mkb' , 1 : 'github' , 2 : 'ckb' , 3 : 'library' , 4 : 'tool' } >>> model DistillBert model Entities embeddings dim 768 Relations embeddings dim 768 Gamma 9.0 Number of entities 5 Number of relations 1 >>> validation = evaluation . Evaluation ( ... entities = dataset . entities , ... relations = dataset . relations , ... true_triples = dataset . train + dataset . valid + dataset . test , ... batch_size = 1 , ... device = 'cpu' , ... ) >>> validation . eval ( model = model , dataset = dataset . valid ) { 'MRR' : 0.3958 , 'MR' : 2.75 , 'HITS@1' : 0.0 , 'HITS@3' : 0.75 , 'HITS@10' : 1.0 }","title":"Examples"},{"location":"api/evaluation/Evaluation/#methods","text":"compute_detailled_score compute_score detail_eval Divide input dataset relations into different categories (i.e. ONE-TO-ONE, ONE-TO-MANY, MANY-TO-ONE and MANY-TO-MANY) according to the mapping properties of relationships. Reference: 1. Bordes, Antoine, et al. \"Translating embeddings for modeling multi-relational data.\" Advances in neural information processing systems. 2013. Parameters model dataset threshold \u2013 defaults to 1.5 eval Evaluate selected model with the metrics: MRR, MR, HITS@1, HITS@3, HITS@10 Parameters model dataset eval_relations Evaluate selected model with the metrics: MRR, MR, HITS@1, HITS@3, HITS@10 Parameters model dataset get_entity_stream Get stream dedicated to link prediction. Parameters dataset get_relation_stream Get stream dedicated to relation prediction. Parameters dataset initialize Initialize model for evaluation Parameters model solve_same_entities Replace artificial entities by the target. Some description may be dedicated to the same entities. Parameters argsort types_relations Divide input dataset relations into different categories (i.e. ONE-TO-ONE, ONE-TO-MANY, MANY-TO-ONE and MANY-TO-MANY) according to the mapping properties of relationships. Parameters model dataset threshold \u2013 defaults to 1.5","title":"Methods"},{"location":"api/models/BaseModel/","text":"BaseModel \u00b6 Base model class. Parameters \u00b6 entities relations hidden_dim scoring gamma Attributes \u00b6 embeddings Extracts embeddings. name Examples \u00b6 >>> from ckb import models >>> from ckb import scoring >>> from mkb import datasets as mkb_datasets >>> import torch >>> _ = torch . manual_seed ( 42 ) >>> dataset = mkb_datasets . CountriesS1 ( 1 ) >>> model = models . BaseModel ( ... entities = dataset . entities , ... relations = dataset . relations , ... hidden_dim = 3 , ... gamma = 3 , ... scoring = scoring . TransE (), ... ) >>> sample = torch . tensor ([[ 3 , 0 , 4 ], [ 5 , 1 , 6 ]]) >>> head , relation , tail , shape = model . batch ( sample ) >>> head [ 'belize' , 'falkland_islands' ] >>> tail [ 'morocco' , 'saint_vincent_and_the_grenadines' ] Methods \u00b6 call Call self as a function. Parameters input kwargs add_module Adds a child module to the current module. The module can be accessed as an attribute using the given name. Args: name (string): name of the child module. The child module can be accessed from this module using the given name module (Module): child module to be added to the module. Parameters name ( str ) module ( Union[ForwardRef('Module'), NoneType] ) apply Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Args: fn (:class: Module -> None): function to be applied to each submodule Returns: Module: self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Parameters fn ( Callable[[ForwardRef('Module')], NoneType] ) batch Process input sample. Parameters sample negative_sample \u2013 defaults to None mode \u2013 defaults to None bfloat16 Casts all floating point parameters and buffers to bfloat16 datatype. Returns: Module: self buffers Returns an iterator over module buffers. Args: recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) (20L,) (20L, 1L, 5L, 5L) Parameters recurse ( bool ) \u2013 defaults to True children Returns an iterator over immediate children modules. Yields: Module: a child module cpu Moves all model parameters and buffers to the CPU. Returns: Module: self cuda Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Args: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self Parameters device ( Union[int, torch.device, NoneType] ) \u2013 defaults to None default_batch distill Default distillation method Parameters sample negative_sample \u2013 defaults to None mode \u2013 defaults to None double Casts all floating point parameters and buffers to double datatype. Returns: Module: self encode Encode input sample, negative sample with respect to the mode. Parameters sample negative_sample \u2013 defaults to None mode \u2013 defaults to None encoder Encoder should be defined in the children class. Parameters e eval Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . Returns: Module: self extra_repr Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. float Casts all floating point parameters and buffers to float datatype. Returns: Module: self format_sample Adapt input tensor to compute scores. Parameters sample negative_sample \u2013 defaults to None forward Compute scores of input sample, negative sample with respect to the mode. Parameters sample negative_sample \u2013 defaults to None mode \u2013 defaults to None half Casts all floating point parameters and buffers to half datatype. Returns: Module: self head_batch Used to get faster when computing scores for negative samples. Parameters sample negative_sample load_state_dict Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Args: state_dict (dict): a dict containing parameters and persistent buffers. strict (bool, optional): whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True Returns: NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Parameters state_dict ( 'OrderedDict[str, Tensor]' ) strict ( bool ) \u2013 defaults to True modules Returns an iterator over all modules in the network. Yields: Module: a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) named_buffers Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) Parameters prefix ( str ) \u2013 defaults to `` recurse ( bool ) \u2013 defaults to True named_children Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) named_modules Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) Parameters memo ( Union[Set[ForwardRef('Module')], NoneType] ) \u2013 defaults to None prefix ( str ) \u2013 defaults to `` named_parameters Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) Parameters prefix ( str ) \u2013 defaults to `` recurse ( bool ) \u2013 defaults to True negative_encoding parameters Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) (20L,) (20L, 1L, 5L, 5L) Parameters recurse ( bool ) \u2013 defaults to True register_backward_hook Registers a backward hook on the module. This function is deprecated in favor of :meth: nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Parameters hook ( Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]] ) register_buffer Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor): buffer to be registered. persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) Parameters name ( str ) tensor ( Union[torch.Tensor, NoneType] ) persistent ( bool ) \u2013 defaults to True register_forward_hook Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func: forward is called. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Parameters hook ( Callable[..., NoneType] ) register_forward_pre_hook Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Parameters hook ( Callable[..., NoneType] ) register_full_backward_hook Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Parameters hook ( Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]] ) register_parameter Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (string): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter): parameter to be added to the module. Parameters name ( str ) param ( Union[torch.nn.parameter.Parameter, NoneType] ) requires_grad_ Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Args: requires_grad (bool): whether autograd should record operations on parameters in this module. Default: True . Returns: Module: self Parameters requires_grad ( bool ) \u2013 defaults to True save share_memory state_dict Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns: dict: a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] Parameters destination \u2013 defaults to None prefix \u2013 defaults to `` keep_vars \u2013 defaults to False tail_batch Used to get faster when computing scores for negative samples. Parameters sample negative_sample to Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) .. function:: to(memory_format=torch.channels_last) Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Args: device (:class: torch.device ): the desired device of the parameters and buffers in this module dtype (:class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module tensor (torch.Tensor): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module memory_format (:class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) Returns: Module: self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) Parameters args kwargs train Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Args: mode (bool): whether to set training mode ( True ) or evaluation mode ( False ). Default: True . Returns: Module: self Parameters mode ( bool ) \u2013 defaults to True type Casts all parameters and buffers to :attr: dst_type . Args: dst_type (type or string): the desired type Returns: Module: self Parameters dst_type ( Union[torch.dtype, str] ) xpu Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. Arguments: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self Parameters device ( Union[int, torch.device, NoneType] ) \u2013 defaults to None zero_grad Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Args: set_to_none (bool): instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. Parameters set_to_none ( bool ) \u2013 defaults to False","title":"BaseModel"},{"location":"api/models/BaseModel/#basemodel","text":"Base model class.","title":"BaseModel"},{"location":"api/models/BaseModel/#parameters","text":"entities relations hidden_dim scoring gamma","title":"Parameters"},{"location":"api/models/BaseModel/#attributes","text":"embeddings Extracts embeddings. name","title":"Attributes"},{"location":"api/models/BaseModel/#examples","text":">>> from ckb import models >>> from ckb import scoring >>> from mkb import datasets as mkb_datasets >>> import torch >>> _ = torch . manual_seed ( 42 ) >>> dataset = mkb_datasets . CountriesS1 ( 1 ) >>> model = models . BaseModel ( ... entities = dataset . entities , ... relations = dataset . relations , ... hidden_dim = 3 , ... gamma = 3 , ... scoring = scoring . TransE (), ... ) >>> sample = torch . tensor ([[ 3 , 0 , 4 ], [ 5 , 1 , 6 ]]) >>> head , relation , tail , shape = model . batch ( sample ) >>> head [ 'belize' , 'falkland_islands' ] >>> tail [ 'morocco' , 'saint_vincent_and_the_grenadines' ]","title":"Examples"},{"location":"api/models/BaseModel/#methods","text":"call Call self as a function. Parameters input kwargs add_module Adds a child module to the current module. The module can be accessed as an attribute using the given name. Args: name (string): name of the child module. The child module can be accessed from this module using the given name module (Module): child module to be added to the module. Parameters name ( str ) module ( Union[ForwardRef('Module'), NoneType] ) apply Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Args: fn (:class: Module -> None): function to be applied to each submodule Returns: Module: self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Parameters fn ( Callable[[ForwardRef('Module')], NoneType] ) batch Process input sample. Parameters sample negative_sample \u2013 defaults to None mode \u2013 defaults to None bfloat16 Casts all floating point parameters and buffers to bfloat16 datatype. Returns: Module: self buffers Returns an iterator over module buffers. Args: recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) (20L,) (20L, 1L, 5L, 5L) Parameters recurse ( bool ) \u2013 defaults to True children Returns an iterator over immediate children modules. Yields: Module: a child module cpu Moves all model parameters and buffers to the CPU. Returns: Module: self cuda Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Args: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self Parameters device ( Union[int, torch.device, NoneType] ) \u2013 defaults to None default_batch distill Default distillation method Parameters sample negative_sample \u2013 defaults to None mode \u2013 defaults to None double Casts all floating point parameters and buffers to double datatype. Returns: Module: self encode Encode input sample, negative sample with respect to the mode. Parameters sample negative_sample \u2013 defaults to None mode \u2013 defaults to None encoder Encoder should be defined in the children class. Parameters e eval Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . Returns: Module: self extra_repr Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. float Casts all floating point parameters and buffers to float datatype. Returns: Module: self format_sample Adapt input tensor to compute scores. Parameters sample negative_sample \u2013 defaults to None forward Compute scores of input sample, negative sample with respect to the mode. Parameters sample negative_sample \u2013 defaults to None mode \u2013 defaults to None half Casts all floating point parameters and buffers to half datatype. Returns: Module: self head_batch Used to get faster when computing scores for negative samples. Parameters sample negative_sample load_state_dict Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Args: state_dict (dict): a dict containing parameters and persistent buffers. strict (bool, optional): whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True Returns: NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Parameters state_dict ( 'OrderedDict[str, Tensor]' ) strict ( bool ) \u2013 defaults to True modules Returns an iterator over all modules in the network. Yields: Module: a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) named_buffers Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) Parameters prefix ( str ) \u2013 defaults to `` recurse ( bool ) \u2013 defaults to True named_children Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) named_modules Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) Parameters memo ( Union[Set[ForwardRef('Module')], NoneType] ) \u2013 defaults to None prefix ( str ) \u2013 defaults to `` named_parameters Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) Parameters prefix ( str ) \u2013 defaults to `` recurse ( bool ) \u2013 defaults to True negative_encoding parameters Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) (20L,) (20L, 1L, 5L, 5L) Parameters recurse ( bool ) \u2013 defaults to True register_backward_hook Registers a backward hook on the module. This function is deprecated in favor of :meth: nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Parameters hook ( Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]] ) register_buffer Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor): buffer to be registered. persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) Parameters name ( str ) tensor ( Union[torch.Tensor, NoneType] ) persistent ( bool ) \u2013 defaults to True register_forward_hook Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func: forward is called. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Parameters hook ( Callable[..., NoneType] ) register_forward_pre_hook Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Parameters hook ( Callable[..., NoneType] ) register_full_backward_hook Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Parameters hook ( Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]] ) register_parameter Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (string): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter): parameter to be added to the module. Parameters name ( str ) param ( Union[torch.nn.parameter.Parameter, NoneType] ) requires_grad_ Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Args: requires_grad (bool): whether autograd should record operations on parameters in this module. Default: True . Returns: Module: self Parameters requires_grad ( bool ) \u2013 defaults to True save share_memory state_dict Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns: dict: a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] Parameters destination \u2013 defaults to None prefix \u2013 defaults to `` keep_vars \u2013 defaults to False tail_batch Used to get faster when computing scores for negative samples. Parameters sample negative_sample to Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) .. function:: to(memory_format=torch.channels_last) Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Args: device (:class: torch.device ): the desired device of the parameters and buffers in this module dtype (:class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module tensor (torch.Tensor): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module memory_format (:class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) Returns: Module: self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) Parameters args kwargs train Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Args: mode (bool): whether to set training mode ( True ) or evaluation mode ( False ). Default: True . Returns: Module: self Parameters mode ( bool ) \u2013 defaults to True type Casts all parameters and buffers to :attr: dst_type . Args: dst_type (type or string): the desired type Returns: Module: self Parameters dst_type ( Union[torch.dtype, str] ) xpu Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. Arguments: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self Parameters device ( Union[int, torch.device, NoneType] ) \u2013 defaults to None zero_grad Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Args: set_to_none (bool): instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. Parameters set_to_none ( bool ) \u2013 defaults to False","title":"Methods"},{"location":"api/models/DistillBert/","text":"DistillBert \u00b6 DistillBert for contextual representation of entities. Parameters \u00b6 entities relations scoring \u2013 defaults to TransE scoring hidden_dim \u2013 defaults to None gamma \u2013 defaults to 9 device \u2013 defaults to cuda Attributes \u00b6 embeddings Extracts embeddings. name Examples \u00b6 >>> from ckb import models >>> from ckb import datasets >>> import torch >>> _ = torch . manual_seed ( 42 ) >>> dataset = datasets . Semanlink ( 1 ) >>> model = models . DistillBert ( ... hidden_dim = 50 , ... entities = dataset . entities , ... relations = dataset . relations , ... gamma = 9 , ... device = 'cpu' , ... ) >>> sample = torch . tensor ([[ 0 , 0 , 0 ], [ 2 , 2 , 2 ]]) >>> model ( sample ) tensor ([[ 3.1645 ], [ 3.2653 ]], grad_fn =< ViewBackward > ) >>> sample = torch . tensor ([[ 0 , 0 , 1 ], [ 2 , 2 , 1 ]]) >>> model ( sample ) tensor ([[ 2.5616 ], [ 0.8435 ]], grad_fn =< ViewBackward > ) >>> sample = torch . tensor ([[ 1 , 0 , 0 ], [ 1 , 2 , 2 ]]) >>> model ( sample ) tensor ([[ 1.1692 ], [ 1.1021 ]], grad_fn =< ViewBackward > ) >>> sample = torch . tensor ([[ 0 , 0 , 0 ], [ 2 , 2 , 2 ]]) >>> negative_sample = torch . tensor ([[ 1 ], [ 1 ]]) >>> model ( sample , negative_sample , mode = 'head-batch' ) tensor ([[ 1.1692 ], [ 1.1021 ]], grad_fn =< ViewBackward > ) >>> model ( sample , negative_sample , mode = 'tail-batch' ) tensor ([[ 2.5616 ], [ 0.8435 ]], grad_fn =< ViewBackward > ) >>> model = models . DistillBert ( ... entities = dataset . entities , ... relations = dataset . relations , ... gamma = 9 , ... device = 'cpu' , ... ) >>> sample = torch . tensor ([[ 0 , 0 , 0 ], [ 2 , 2 , 2 ]]) >>> model ( sample ) tensor ([[ 3.6504 ], [ 3.3879 ]], grad_fn =< ViewBackward > ) Methods \u00b6 call Call self as a function. Parameters input kwargs add_module Adds a child module to the current module. The module can be accessed as an attribute using the given name. Args: name (string): name of the child module. The child module can be accessed from this module using the given name module (Module): child module to be added to the module. Parameters name ( str ) module ( Union[ForwardRef('Module'), NoneType] ) apply Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Args: fn (:class: Module -> None): function to be applied to each submodule Returns: Module: self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Parameters fn ( Callable[[ForwardRef('Module')], NoneType] ) batch Process input sample. Parameters sample negative_sample \u2013 defaults to None mode \u2013 defaults to None bfloat16 Casts all floating point parameters and buffers to bfloat16 datatype. Returns: Module: self buffers Returns an iterator over module buffers. Args: recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) (20L,) (20L, 1L, 5L, 5L) Parameters recurse ( bool ) \u2013 defaults to True children Returns an iterator over immediate children modules. Yields: Module: a child module cpu Moves all model parameters and buffers to the CPU. Returns: Module: self cuda Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Args: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self Parameters device ( Union[int, torch.device, NoneType] ) \u2013 defaults to None default_batch distill Default distillation method Parameters sample negative_sample \u2013 defaults to None mode \u2013 defaults to None double Casts all floating point parameters and buffers to double datatype. Returns: Module: self encode Encode input sample, negative sample with respect to the mode. Parameters sample negative_sample \u2013 defaults to None mode \u2013 defaults to None encoder Encode input entities descriptions. Parameters: e (list): List of description of entities. Returns: Torch tensor of encoded entities. Parameters e eval Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . Returns: Module: self extra_repr Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. float Casts all floating point parameters and buffers to float datatype. Returns: Module: self format_sample Adapt input tensor to compute scores. Parameters sample negative_sample \u2013 defaults to None forward Compute scores of input sample, negative sample with respect to the mode. Parameters sample negative_sample \u2013 defaults to None mode \u2013 defaults to None half Casts all floating point parameters and buffers to half datatype. Returns: Module: self head_batch Used to get faster when computing scores for negative samples. Parameters sample negative_sample load_state_dict Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Args: state_dict (dict): a dict containing parameters and persistent buffers. strict (bool, optional): whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True Returns: NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Parameters state_dict ( 'OrderedDict[str, Tensor]' ) strict ( bool ) \u2013 defaults to True modules Returns an iterator over all modules in the network. Yields: Module: a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) named_buffers Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) Parameters prefix ( str ) \u2013 defaults to `` recurse ( bool ) \u2013 defaults to True named_children Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) named_modules Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) Parameters memo ( Union[Set[ForwardRef('Module')], NoneType] ) \u2013 defaults to None prefix ( str ) \u2013 defaults to `` named_parameters Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) Parameters prefix ( str ) \u2013 defaults to `` recurse ( bool ) \u2013 defaults to True negative_encoding parameters Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) (20L,) (20L, 1L, 5L, 5L) Parameters recurse ( bool ) \u2013 defaults to True register_backward_hook Registers a backward hook on the module. This function is deprecated in favor of :meth: nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Parameters hook ( Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]] ) register_buffer Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor): buffer to be registered. persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) Parameters name ( str ) tensor ( Union[torch.Tensor, NoneType] ) persistent ( bool ) \u2013 defaults to True register_forward_hook Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func: forward is called. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Parameters hook ( Callable[..., NoneType] ) register_forward_pre_hook Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Parameters hook ( Callable[..., NoneType] ) register_full_backward_hook Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Parameters hook ( Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]] ) register_parameter Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (string): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter): parameter to be added to the module. Parameters name ( str ) param ( Union[torch.nn.parameter.Parameter, NoneType] ) requires_grad_ Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Args: requires_grad (bool): whether autograd should record operations on parameters in this module. Default: True . Returns: Module: self Parameters requires_grad ( bool ) \u2013 defaults to True save share_memory state_dict Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns: dict: a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] Parameters destination \u2013 defaults to None prefix \u2013 defaults to `` keep_vars \u2013 defaults to False tail_batch Used to get faster when computing scores for negative samples. Parameters sample negative_sample to Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) .. function:: to(memory_format=torch.channels_last) Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Args: device (:class: torch.device ): the desired device of the parameters and buffers in this module dtype (:class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module tensor (torch.Tensor): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module memory_format (:class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) Returns: Module: self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) Parameters args kwargs train Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Args: mode (bool): whether to set training mode ( True ) or evaluation mode ( False ). Default: True . Returns: Module: self Parameters mode ( bool ) \u2013 defaults to True type Casts all parameters and buffers to :attr: dst_type . Args: dst_type (type or string): the desired type Returns: Module: self Parameters dst_type ( Union[torch.dtype, str] ) xpu Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. Arguments: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self Parameters device ( Union[int, torch.device, NoneType] ) \u2013 defaults to None zero_grad Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Args: set_to_none (bool): instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. Parameters set_to_none ( bool ) \u2013 defaults to False","title":"DistillBert"},{"location":"api/models/DistillBert/#distillbert","text":"DistillBert for contextual representation of entities.","title":"DistillBert"},{"location":"api/models/DistillBert/#parameters","text":"entities relations scoring \u2013 defaults to TransE scoring hidden_dim \u2013 defaults to None gamma \u2013 defaults to 9 device \u2013 defaults to cuda","title":"Parameters"},{"location":"api/models/DistillBert/#attributes","text":"embeddings Extracts embeddings. name","title":"Attributes"},{"location":"api/models/DistillBert/#examples","text":">>> from ckb import models >>> from ckb import datasets >>> import torch >>> _ = torch . manual_seed ( 42 ) >>> dataset = datasets . Semanlink ( 1 ) >>> model = models . DistillBert ( ... hidden_dim = 50 , ... entities = dataset . entities , ... relations = dataset . relations , ... gamma = 9 , ... device = 'cpu' , ... ) >>> sample = torch . tensor ([[ 0 , 0 , 0 ], [ 2 , 2 , 2 ]]) >>> model ( sample ) tensor ([[ 3.1645 ], [ 3.2653 ]], grad_fn =< ViewBackward > ) >>> sample = torch . tensor ([[ 0 , 0 , 1 ], [ 2 , 2 , 1 ]]) >>> model ( sample ) tensor ([[ 2.5616 ], [ 0.8435 ]], grad_fn =< ViewBackward > ) >>> sample = torch . tensor ([[ 1 , 0 , 0 ], [ 1 , 2 , 2 ]]) >>> model ( sample ) tensor ([[ 1.1692 ], [ 1.1021 ]], grad_fn =< ViewBackward > ) >>> sample = torch . tensor ([[ 0 , 0 , 0 ], [ 2 , 2 , 2 ]]) >>> negative_sample = torch . tensor ([[ 1 ], [ 1 ]]) >>> model ( sample , negative_sample , mode = 'head-batch' ) tensor ([[ 1.1692 ], [ 1.1021 ]], grad_fn =< ViewBackward > ) >>> model ( sample , negative_sample , mode = 'tail-batch' ) tensor ([[ 2.5616 ], [ 0.8435 ]], grad_fn =< ViewBackward > ) >>> model = models . DistillBert ( ... entities = dataset . entities , ... relations = dataset . relations , ... gamma = 9 , ... device = 'cpu' , ... ) >>> sample = torch . tensor ([[ 0 , 0 , 0 ], [ 2 , 2 , 2 ]]) >>> model ( sample ) tensor ([[ 3.6504 ], [ 3.3879 ]], grad_fn =< ViewBackward > )","title":"Examples"},{"location":"api/models/DistillBert/#methods","text":"call Call self as a function. Parameters input kwargs add_module Adds a child module to the current module. The module can be accessed as an attribute using the given name. Args: name (string): name of the child module. The child module can be accessed from this module using the given name module (Module): child module to be added to the module. Parameters name ( str ) module ( Union[ForwardRef('Module'), NoneType] ) apply Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Args: fn (:class: Module -> None): function to be applied to each submodule Returns: Module: self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Parameters fn ( Callable[[ForwardRef('Module')], NoneType] ) batch Process input sample. Parameters sample negative_sample \u2013 defaults to None mode \u2013 defaults to None bfloat16 Casts all floating point parameters and buffers to bfloat16 datatype. Returns: Module: self buffers Returns an iterator over module buffers. Args: recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) (20L,) (20L, 1L, 5L, 5L) Parameters recurse ( bool ) \u2013 defaults to True children Returns an iterator over immediate children modules. Yields: Module: a child module cpu Moves all model parameters and buffers to the CPU. Returns: Module: self cuda Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Args: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self Parameters device ( Union[int, torch.device, NoneType] ) \u2013 defaults to None default_batch distill Default distillation method Parameters sample negative_sample \u2013 defaults to None mode \u2013 defaults to None double Casts all floating point parameters and buffers to double datatype. Returns: Module: self encode Encode input sample, negative sample with respect to the mode. Parameters sample negative_sample \u2013 defaults to None mode \u2013 defaults to None encoder Encode input entities descriptions. Parameters: e (list): List of description of entities. Returns: Torch tensor of encoded entities. Parameters e eval Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . Returns: Module: self extra_repr Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. float Casts all floating point parameters and buffers to float datatype. Returns: Module: self format_sample Adapt input tensor to compute scores. Parameters sample negative_sample \u2013 defaults to None forward Compute scores of input sample, negative sample with respect to the mode. Parameters sample negative_sample \u2013 defaults to None mode \u2013 defaults to None half Casts all floating point parameters and buffers to half datatype. Returns: Module: self head_batch Used to get faster when computing scores for negative samples. Parameters sample negative_sample load_state_dict Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Args: state_dict (dict): a dict containing parameters and persistent buffers. strict (bool, optional): whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True Returns: NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Parameters state_dict ( 'OrderedDict[str, Tensor]' ) strict ( bool ) \u2013 defaults to True modules Returns an iterator over all modules in the network. Yields: Module: a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) named_buffers Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) Parameters prefix ( str ) \u2013 defaults to `` recurse ( bool ) \u2013 defaults to True named_children Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) named_modules Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) Parameters memo ( Union[Set[ForwardRef('Module')], NoneType] ) \u2013 defaults to None prefix ( str ) \u2013 defaults to `` named_parameters Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) Parameters prefix ( str ) \u2013 defaults to `` recurse ( bool ) \u2013 defaults to True negative_encoding parameters Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) (20L,) (20L, 1L, 5L, 5L) Parameters recurse ( bool ) \u2013 defaults to True register_backward_hook Registers a backward hook on the module. This function is deprecated in favor of :meth: nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Parameters hook ( Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]] ) register_buffer Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor): buffer to be registered. persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) Parameters name ( str ) tensor ( Union[torch.Tensor, NoneType] ) persistent ( bool ) \u2013 defaults to True register_forward_hook Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func: forward is called. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Parameters hook ( Callable[..., NoneType] ) register_forward_pre_hook Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Parameters hook ( Callable[..., NoneType] ) register_full_backward_hook Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Parameters hook ( Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]] ) register_parameter Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (string): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter): parameter to be added to the module. Parameters name ( str ) param ( Union[torch.nn.parameter.Parameter, NoneType] ) requires_grad_ Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Args: requires_grad (bool): whether autograd should record operations on parameters in this module. Default: True . Returns: Module: self Parameters requires_grad ( bool ) \u2013 defaults to True save share_memory state_dict Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns: dict: a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] Parameters destination \u2013 defaults to None prefix \u2013 defaults to `` keep_vars \u2013 defaults to False tail_batch Used to get faster when computing scores for negative samples. Parameters sample negative_sample to Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) .. function:: to(memory_format=torch.channels_last) Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Args: device (:class: torch.device ): the desired device of the parameters and buffers in this module dtype (:class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module tensor (torch.Tensor): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module memory_format (:class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) Returns: Module: self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) Parameters args kwargs train Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Args: mode (bool): whether to set training mode ( True ) or evaluation mode ( False ). Default: True . Returns: Module: self Parameters mode ( bool ) \u2013 defaults to True type Casts all parameters and buffers to :attr: dst_type . Args: dst_type (type or string): the desired type Returns: Module: self Parameters dst_type ( Union[torch.dtype, str] ) xpu Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. Arguments: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self Parameters device ( Union[int, torch.device, NoneType] ) \u2013 defaults to None zero_grad Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Args: set_to_none (bool): instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. Parameters set_to_none ( bool ) \u2013 defaults to False","title":"Methods"},{"location":"api/models/FlauBERT/","text":"FlauBERT \u00b6 FlauBERT for contextual representation of entities. Parameters \u00b6 entities relations scoring \u2013 defaults to TransE scoring hidden_dim \u2013 defaults to None gamma \u2013 defaults to 9 device \u2013 defaults to cuda Attributes \u00b6 embeddings Extracts embeddings. name Examples \u00b6 >>> from ckb import models >>> from ckb import datasets >>> import torch >>> _ = torch . manual_seed ( 42 ) >>> dataset = datasets . Semanlink ( 1 ) >>> model = models . FlauBERT ( ... hidden_dim = 50 , ... entities = dataset . entities , ... relations = dataset . relations , ... gamma = 9 , ... device = 'cpu' , ... ) >>> model FlauBERT model Entities embeddings dim 50 Relations embeddings dim 50 Gamma 9.0 Number of entities 5454 Number of relations 4 Methods \u00b6 call Call self as a function. Parameters input kwargs add_module Adds a child module to the current module. The module can be accessed as an attribute using the given name. Args: name (string): name of the child module. The child module can be accessed from this module using the given name module (Module): child module to be added to the module. Parameters name ( str ) module ( Union[ForwardRef('Module'), NoneType] ) apply Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Args: fn (:class: Module -> None): function to be applied to each submodule Returns: Module: self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Parameters fn ( Callable[[ForwardRef('Module')], NoneType] ) batch Process input sample. Parameters sample negative_sample \u2013 defaults to None mode \u2013 defaults to None bfloat16 Casts all floating point parameters and buffers to bfloat16 datatype. Returns: Module: self buffers Returns an iterator over module buffers. Args: recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) (20L,) (20L, 1L, 5L, 5L) Parameters recurse ( bool ) \u2013 defaults to True children Returns an iterator over immediate children modules. Yields: Module: a child module cpu Moves all model parameters and buffers to the CPU. Returns: Module: self cuda Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Args: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self Parameters device ( Union[int, torch.device, NoneType] ) \u2013 defaults to None default_batch distill Default distillation method Parameters sample negative_sample \u2013 defaults to None mode \u2013 defaults to None double Casts all floating point parameters and buffers to double datatype. Returns: Module: self encode Encode input sample, negative sample with respect to the mode. Parameters sample negative_sample \u2013 defaults to None mode \u2013 defaults to None encoder Encode input entities descriptions. Parameters: e (list): List of description of entities. Returns: Torch tensor of encoded entities. Parameters e eval Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . Returns: Module: self extra_repr Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. float Casts all floating point parameters and buffers to float datatype. Returns: Module: self format_sample Adapt input tensor to compute scores. Parameters sample negative_sample \u2013 defaults to None forward Compute scores of input sample, negative sample with respect to the mode. Parameters sample negative_sample \u2013 defaults to None mode \u2013 defaults to None half Casts all floating point parameters and buffers to half datatype. Returns: Module: self head_batch Used to get faster when computing scores for negative samples. Parameters sample negative_sample load_state_dict Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Args: state_dict (dict): a dict containing parameters and persistent buffers. strict (bool, optional): whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True Returns: NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Parameters state_dict ( 'OrderedDict[str, Tensor]' ) strict ( bool ) \u2013 defaults to True modules Returns an iterator over all modules in the network. Yields: Module: a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) named_buffers Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) Parameters prefix ( str ) \u2013 defaults to `` recurse ( bool ) \u2013 defaults to True named_children Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) named_modules Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) Parameters memo ( Union[Set[ForwardRef('Module')], NoneType] ) \u2013 defaults to None prefix ( str ) \u2013 defaults to `` named_parameters Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) Parameters prefix ( str ) \u2013 defaults to `` recurse ( bool ) \u2013 defaults to True negative_encoding parameters Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) (20L,) (20L, 1L, 5L, 5L) Parameters recurse ( bool ) \u2013 defaults to True register_backward_hook Registers a backward hook on the module. This function is deprecated in favor of :meth: nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Parameters hook ( Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]] ) register_buffer Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor): buffer to be registered. persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) Parameters name ( str ) tensor ( Union[torch.Tensor, NoneType] ) persistent ( bool ) \u2013 defaults to True register_forward_hook Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func: forward is called. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Parameters hook ( Callable[..., NoneType] ) register_forward_pre_hook Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Parameters hook ( Callable[..., NoneType] ) register_full_backward_hook Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Parameters hook ( Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]] ) register_parameter Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (string): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter): parameter to be added to the module. Parameters name ( str ) param ( Union[torch.nn.parameter.Parameter, NoneType] ) requires_grad_ Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Args: requires_grad (bool): whether autograd should record operations on parameters in this module. Default: True . Returns: Module: self Parameters requires_grad ( bool ) \u2013 defaults to True save share_memory state_dict Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns: dict: a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] Parameters destination \u2013 defaults to None prefix \u2013 defaults to `` keep_vars \u2013 defaults to False tail_batch Used to get faster when computing scores for negative samples. Parameters sample negative_sample to Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) .. function:: to(memory_format=torch.channels_last) Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Args: device (:class: torch.device ): the desired device of the parameters and buffers in this module dtype (:class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module tensor (torch.Tensor): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module memory_format (:class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) Returns: Module: self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) Parameters args kwargs train Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Args: mode (bool): whether to set training mode ( True ) or evaluation mode ( False ). Default: True . Returns: Module: self Parameters mode ( bool ) \u2013 defaults to True type Casts all parameters and buffers to :attr: dst_type . Args: dst_type (type or string): the desired type Returns: Module: self Parameters dst_type ( Union[torch.dtype, str] ) xpu Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. Arguments: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self Parameters device ( Union[int, torch.device, NoneType] ) \u2013 defaults to None zero_grad Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Args: set_to_none (bool): instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. Parameters set_to_none ( bool ) \u2013 defaults to False","title":"FlauBERT"},{"location":"api/models/FlauBERT/#flaubert","text":"FlauBERT for contextual representation of entities.","title":"FlauBERT"},{"location":"api/models/FlauBERT/#parameters","text":"entities relations scoring \u2013 defaults to TransE scoring hidden_dim \u2013 defaults to None gamma \u2013 defaults to 9 device \u2013 defaults to cuda","title":"Parameters"},{"location":"api/models/FlauBERT/#attributes","text":"embeddings Extracts embeddings. name","title":"Attributes"},{"location":"api/models/FlauBERT/#examples","text":">>> from ckb import models >>> from ckb import datasets >>> import torch >>> _ = torch . manual_seed ( 42 ) >>> dataset = datasets . Semanlink ( 1 ) >>> model = models . FlauBERT ( ... hidden_dim = 50 , ... entities = dataset . entities , ... relations = dataset . relations , ... gamma = 9 , ... device = 'cpu' , ... ) >>> model FlauBERT model Entities embeddings dim 50 Relations embeddings dim 50 Gamma 9.0 Number of entities 5454 Number of relations 4","title":"Examples"},{"location":"api/models/FlauBERT/#methods","text":"call Call self as a function. Parameters input kwargs add_module Adds a child module to the current module. The module can be accessed as an attribute using the given name. Args: name (string): name of the child module. The child module can be accessed from this module using the given name module (Module): child module to be added to the module. Parameters name ( str ) module ( Union[ForwardRef('Module'), NoneType] ) apply Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Args: fn (:class: Module -> None): function to be applied to each submodule Returns: Module: self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Parameters fn ( Callable[[ForwardRef('Module')], NoneType] ) batch Process input sample. Parameters sample negative_sample \u2013 defaults to None mode \u2013 defaults to None bfloat16 Casts all floating point parameters and buffers to bfloat16 datatype. Returns: Module: self buffers Returns an iterator over module buffers. Args: recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) (20L,) (20L, 1L, 5L, 5L) Parameters recurse ( bool ) \u2013 defaults to True children Returns an iterator over immediate children modules. Yields: Module: a child module cpu Moves all model parameters and buffers to the CPU. Returns: Module: self cuda Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Args: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self Parameters device ( Union[int, torch.device, NoneType] ) \u2013 defaults to None default_batch distill Default distillation method Parameters sample negative_sample \u2013 defaults to None mode \u2013 defaults to None double Casts all floating point parameters and buffers to double datatype. Returns: Module: self encode Encode input sample, negative sample with respect to the mode. Parameters sample negative_sample \u2013 defaults to None mode \u2013 defaults to None encoder Encode input entities descriptions. Parameters: e (list): List of description of entities. Returns: Torch tensor of encoded entities. Parameters e eval Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . Returns: Module: self extra_repr Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. float Casts all floating point parameters and buffers to float datatype. Returns: Module: self format_sample Adapt input tensor to compute scores. Parameters sample negative_sample \u2013 defaults to None forward Compute scores of input sample, negative sample with respect to the mode. Parameters sample negative_sample \u2013 defaults to None mode \u2013 defaults to None half Casts all floating point parameters and buffers to half datatype. Returns: Module: self head_batch Used to get faster when computing scores for negative samples. Parameters sample negative_sample load_state_dict Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Args: state_dict (dict): a dict containing parameters and persistent buffers. strict (bool, optional): whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True Returns: NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Parameters state_dict ( 'OrderedDict[str, Tensor]' ) strict ( bool ) \u2013 defaults to True modules Returns an iterator over all modules in the network. Yields: Module: a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) named_buffers Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) Parameters prefix ( str ) \u2013 defaults to `` recurse ( bool ) \u2013 defaults to True named_children Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) named_modules Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) Parameters memo ( Union[Set[ForwardRef('Module')], NoneType] ) \u2013 defaults to None prefix ( str ) \u2013 defaults to `` named_parameters Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) Parameters prefix ( str ) \u2013 defaults to `` recurse ( bool ) \u2013 defaults to True negative_encoding parameters Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) (20L,) (20L, 1L, 5L, 5L) Parameters recurse ( bool ) \u2013 defaults to True register_backward_hook Registers a backward hook on the module. This function is deprecated in favor of :meth: nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Parameters hook ( Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]] ) register_buffer Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor): buffer to be registered. persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) Parameters name ( str ) tensor ( Union[torch.Tensor, NoneType] ) persistent ( bool ) \u2013 defaults to True register_forward_hook Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func: forward is called. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Parameters hook ( Callable[..., NoneType] ) register_forward_pre_hook Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Parameters hook ( Callable[..., NoneType] ) register_full_backward_hook Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Parameters hook ( Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]] ) register_parameter Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (string): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter): parameter to be added to the module. Parameters name ( str ) param ( Union[torch.nn.parameter.Parameter, NoneType] ) requires_grad_ Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Args: requires_grad (bool): whether autograd should record operations on parameters in this module. Default: True . Returns: Module: self Parameters requires_grad ( bool ) \u2013 defaults to True save share_memory state_dict Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns: dict: a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] Parameters destination \u2013 defaults to None prefix \u2013 defaults to `` keep_vars \u2013 defaults to False tail_batch Used to get faster when computing scores for negative samples. Parameters sample negative_sample to Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) .. function:: to(memory_format=torch.channels_last) Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Args: device (:class: torch.device ): the desired device of the parameters and buffers in this module dtype (:class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module tensor (torch.Tensor): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module memory_format (:class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) Returns: Module: self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) Parameters args kwargs train Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Args: mode (bool): whether to set training mode ( True ) or evaluation mode ( False ). Default: True . Returns: Module: self Parameters mode ( bool ) \u2013 defaults to True type Casts all parameters and buffers to :attr: dst_type . Args: dst_type (type or string): the desired type Returns: Module: self Parameters dst_type ( Union[torch.dtype, str] ) xpu Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. Arguments: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self Parameters device ( Union[int, torch.device, NoneType] ) \u2013 defaults to None zero_grad Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Args: set_to_none (bool): instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. Parameters set_to_none ( bool ) \u2013 defaults to False","title":"Methods"},{"location":"api/models/Transformer/","text":"Transformer \u00b6 Transformer for contextual representation of entities. Parameters \u00b6 model tokenizer entities relations scoring \u2013 defaults to TransE scoring hidden_dim \u2013 defaults to None gamma \u2013 defaults to 9 device \u2013 defaults to cuda Attributes \u00b6 embeddings Extracts embeddings. name Examples \u00b6 >>> import torch >>> _ = torch . manual_seed ( 42 ) >>> from ckb import models >>> from ckb import datasets >>> from transformers import BertTokenizer >>> from transformers import BertModel >>> dataset = datasets . Semanlink ( 1 ) >>> model = models . Transformer ( ... model = BertModel . from_pretrained ( 'bert-base-uncased' ), ... tokenizer = BertTokenizer . from_pretrained ( 'bert-base-uncased' ), ... entities = dataset . entities , ... relations = dataset . relations , ... gamma = 9 , ... device = 'cpu' , ... ) >>> sample = torch . tensor ([[ 0 , 0 , 0 ], [ 2 , 2 , 2 ]]) >>> model ( sample ) tensor ([[ 3.5500 ], [ 3.2861 ]], grad_fn =< ViewBackward > ) >>> sample = torch . tensor ([[ 0 , 0 , 1 ], [ 2 , 2 , 1 ]]) >>> model ( sample ) tensor ([[ - 227.8486 ], [ - 197.0484 ]], grad_fn =< ViewBackward > ) >>> sample = torch . tensor ([[ 1 , 0 , 0 ], [ 1 , 2 , 2 ]]) >>> model ( sample ) tensor ([[ - 227.8378 ], [ - 196.5193 ]], grad_fn =< ViewBackward > ) >>> sample = torch . tensor ([[ 0 , 0 , 0 ], [ 2 , 2 , 2 ]]) >>> negative_sample = torch . tensor ([[ 1 ], [ 1 ]]) >>> model ( sample , negative_sample , mode = 'head-batch' ) tensor ([[ - 227.8378 ], [ - 196.5193 ]], grad_fn =< ViewBackward > ) >>> model ( sample , negative_sample , mode = 'tail-batch' ) tensor ([[ - 227.8486 ], [ - 197.0484 ]], grad_fn =< ViewBackward > ) Methods \u00b6 call Call self as a function. Parameters input kwargs add_module Adds a child module to the current module. The module can be accessed as an attribute using the given name. Args: name (string): name of the child module. The child module can be accessed from this module using the given name module (Module): child module to be added to the module. Parameters name ( str ) module ( Union[ForwardRef('Module'), NoneType] ) apply Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Args: fn (:class: Module -> None): function to be applied to each submodule Returns: Module: self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Parameters fn ( Callable[[ForwardRef('Module')], NoneType] ) batch Process input sample. Parameters sample negative_sample \u2013 defaults to None mode \u2013 defaults to None bfloat16 Casts all floating point parameters and buffers to bfloat16 datatype. Returns: Module: self buffers Returns an iterator over module buffers. Args: recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) (20L,) (20L, 1L, 5L, 5L) Parameters recurse ( bool ) \u2013 defaults to True children Returns an iterator over immediate children modules. Yields: Module: a child module cpu Moves all model parameters and buffers to the CPU. Returns: Module: self cuda Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Args: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self Parameters device ( Union[int, torch.device, NoneType] ) \u2013 defaults to None default_batch distill Default distillation method Parameters sample negative_sample \u2013 defaults to None mode \u2013 defaults to None double Casts all floating point parameters and buffers to double datatype. Returns: Module: self encode Encode input sample, negative sample with respect to the mode. Parameters sample negative_sample \u2013 defaults to None mode \u2013 defaults to None encoder Encode input entities descriptions. Parameters: e (list): List of description of entities. Returns: Torch tensor of encoded entities. Parameters e eval Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . Returns: Module: self extra_repr Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. float Casts all floating point parameters and buffers to float datatype. Returns: Module: self format_sample Adapt input tensor to compute scores. Parameters sample negative_sample \u2013 defaults to None forward Compute scores of input sample, negative sample with respect to the mode. Parameters sample negative_sample \u2013 defaults to None mode \u2013 defaults to None half Casts all floating point parameters and buffers to half datatype. Returns: Module: self head_batch Used to get faster when computing scores for negative samples. Parameters sample negative_sample load_state_dict Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Args: state_dict (dict): a dict containing parameters and persistent buffers. strict (bool, optional): whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True Returns: NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Parameters state_dict ( 'OrderedDict[str, Tensor]' ) strict ( bool ) \u2013 defaults to True modules Returns an iterator over all modules in the network. Yields: Module: a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) named_buffers Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) Parameters prefix ( str ) \u2013 defaults to `` recurse ( bool ) \u2013 defaults to True named_children Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) named_modules Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) Parameters memo ( Union[Set[ForwardRef('Module')], NoneType] ) \u2013 defaults to None prefix ( str ) \u2013 defaults to `` named_parameters Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) Parameters prefix ( str ) \u2013 defaults to `` recurse ( bool ) \u2013 defaults to True negative_encoding parameters Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) (20L,) (20L, 1L, 5L, 5L) Parameters recurse ( bool ) \u2013 defaults to True register_backward_hook Registers a backward hook on the module. This function is deprecated in favor of :meth: nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Parameters hook ( Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]] ) register_buffer Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor): buffer to be registered. persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) Parameters name ( str ) tensor ( Union[torch.Tensor, NoneType] ) persistent ( bool ) \u2013 defaults to True register_forward_hook Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func: forward is called. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Parameters hook ( Callable[..., NoneType] ) register_forward_pre_hook Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Parameters hook ( Callable[..., NoneType] ) register_full_backward_hook Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Parameters hook ( Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]] ) register_parameter Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (string): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter): parameter to be added to the module. Parameters name ( str ) param ( Union[torch.nn.parameter.Parameter, NoneType] ) requires_grad_ Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Args: requires_grad (bool): whether autograd should record operations on parameters in this module. Default: True . Returns: Module: self Parameters requires_grad ( bool ) \u2013 defaults to True save share_memory state_dict Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns: dict: a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] Parameters destination \u2013 defaults to None prefix \u2013 defaults to `` keep_vars \u2013 defaults to False tail_batch Used to get faster when computing scores for negative samples. Parameters sample negative_sample to Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) .. function:: to(memory_format=torch.channels_last) Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Args: device (:class: torch.device ): the desired device of the parameters and buffers in this module dtype (:class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module tensor (torch.Tensor): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module memory_format (:class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) Returns: Module: self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) Parameters args kwargs train Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Args: mode (bool): whether to set training mode ( True ) or evaluation mode ( False ). Default: True . Returns: Module: self Parameters mode ( bool ) \u2013 defaults to True type Casts all parameters and buffers to :attr: dst_type . Args: dst_type (type or string): the desired type Returns: Module: self Parameters dst_type ( Union[torch.dtype, str] ) xpu Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. Arguments: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self Parameters device ( Union[int, torch.device, NoneType] ) \u2013 defaults to None zero_grad Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Args: set_to_none (bool): instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. Parameters set_to_none ( bool ) \u2013 defaults to False","title":"Transformer"},{"location":"api/models/Transformer/#transformer","text":"Transformer for contextual representation of entities.","title":"Transformer"},{"location":"api/models/Transformer/#parameters","text":"model tokenizer entities relations scoring \u2013 defaults to TransE scoring hidden_dim \u2013 defaults to None gamma \u2013 defaults to 9 device \u2013 defaults to cuda","title":"Parameters"},{"location":"api/models/Transformer/#attributes","text":"embeddings Extracts embeddings. name","title":"Attributes"},{"location":"api/models/Transformer/#examples","text":">>> import torch >>> _ = torch . manual_seed ( 42 ) >>> from ckb import models >>> from ckb import datasets >>> from transformers import BertTokenizer >>> from transformers import BertModel >>> dataset = datasets . Semanlink ( 1 ) >>> model = models . Transformer ( ... model = BertModel . from_pretrained ( 'bert-base-uncased' ), ... tokenizer = BertTokenizer . from_pretrained ( 'bert-base-uncased' ), ... entities = dataset . entities , ... relations = dataset . relations , ... gamma = 9 , ... device = 'cpu' , ... ) >>> sample = torch . tensor ([[ 0 , 0 , 0 ], [ 2 , 2 , 2 ]]) >>> model ( sample ) tensor ([[ 3.5500 ], [ 3.2861 ]], grad_fn =< ViewBackward > ) >>> sample = torch . tensor ([[ 0 , 0 , 1 ], [ 2 , 2 , 1 ]]) >>> model ( sample ) tensor ([[ - 227.8486 ], [ - 197.0484 ]], grad_fn =< ViewBackward > ) >>> sample = torch . tensor ([[ 1 , 0 , 0 ], [ 1 , 2 , 2 ]]) >>> model ( sample ) tensor ([[ - 227.8378 ], [ - 196.5193 ]], grad_fn =< ViewBackward > ) >>> sample = torch . tensor ([[ 0 , 0 , 0 ], [ 2 , 2 , 2 ]]) >>> negative_sample = torch . tensor ([[ 1 ], [ 1 ]]) >>> model ( sample , negative_sample , mode = 'head-batch' ) tensor ([[ - 227.8378 ], [ - 196.5193 ]], grad_fn =< ViewBackward > ) >>> model ( sample , negative_sample , mode = 'tail-batch' ) tensor ([[ - 227.8486 ], [ - 197.0484 ]], grad_fn =< ViewBackward > )","title":"Examples"},{"location":"api/models/Transformer/#methods","text":"call Call self as a function. Parameters input kwargs add_module Adds a child module to the current module. The module can be accessed as an attribute using the given name. Args: name (string): name of the child module. The child module can be accessed from this module using the given name module (Module): child module to be added to the module. Parameters name ( str ) module ( Union[ForwardRef('Module'), NoneType] ) apply Applies fn recursively to every submodule (as returned by .children() ) as well as self. Typical use includes initializing the parameters of a model (see also :ref: nn-init-doc ). Args: fn (:class: Module -> None): function to be applied to each submodule Returns: Module: self Example:: >>> @torch.no_grad() >>> def init_weights(m): >>> print(m) >>> if type(m) == nn.Linear: >>> m.weight.fill_(1.0) >>> print(m.weight) >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2)) >>> net.apply(init_weights) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Linear(in_features=2, out_features=2, bias=True) Parameter containing: tensor([[ 1., 1.], [ 1., 1.]]) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) Parameters fn ( Callable[[ForwardRef('Module')], NoneType] ) batch Process input sample. Parameters sample negative_sample \u2013 defaults to None mode \u2013 defaults to None bfloat16 Casts all floating point parameters and buffers to bfloat16 datatype. Returns: Module: self buffers Returns an iterator over module buffers. Args: recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: torch.Tensor: module buffer Example:: >>> for buf in model.buffers(): >>> print(type(buf), buf.size()) (20L,) (20L, 1L, 5L, 5L) Parameters recurse ( bool ) \u2013 defaults to True children Returns an iterator over immediate children modules. Yields: Module: a child module cpu Moves all model parameters and buffers to the CPU. Returns: Module: self cuda Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized. Args: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self Parameters device ( Union[int, torch.device, NoneType] ) \u2013 defaults to None default_batch distill Default distillation method Parameters sample negative_sample \u2013 defaults to None mode \u2013 defaults to None double Casts all floating point parameters and buffers to double datatype. Returns: Module: self encode Encode input sample, negative sample with respect to the mode. Parameters sample negative_sample \u2013 defaults to None mode \u2013 defaults to None encoder Encode input entities descriptions. Parameters: e (list): List of description of entities. Returns: Torch tensor of encoded entities. Parameters e eval Sets the module in evaluation mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. This is equivalent with :meth: self.train(False) <torch.nn.Module.train> . Returns: Module: self extra_repr Set the extra representation of the module To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable. float Casts all floating point parameters and buffers to float datatype. Returns: Module: self format_sample Adapt input tensor to compute scores. Parameters sample negative_sample \u2013 defaults to None forward Compute scores of input sample, negative sample with respect to the mode. Parameters sample negative_sample \u2013 defaults to None mode \u2013 defaults to None half Casts all floating point parameters and buffers to half datatype. Returns: Module: self head_batch Used to get faster when computing scores for negative samples. Parameters sample negative_sample load_state_dict Copies parameters and buffers from :attr: state_dict into this module and its descendants. If :attr: strict is True , then the keys of :attr: state_dict must exactly match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Args: state_dict (dict): a dict containing parameters and persistent buffers. strict (bool, optional): whether to strictly enforce that the keys in :attr: state_dict match the keys returned by this module's :meth: ~torch.nn.Module.state_dict function. Default: True Returns: NamedTuple with missing_keys and unexpected_keys fields: * missing_keys is a list of str containing the missing keys * unexpected_keys is a list of str containing the unexpected keys Parameters state_dict ( 'OrderedDict[str, Tensor]' ) strict ( bool ) \u2013 defaults to True modules Returns an iterator over all modules in the network. Yields: Module: a module in the network Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.modules()): print(idx, '->', m) 0 -> Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) ) 1 -> Linear(in_features=2, out_features=2, bias=True) named_buffers Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself. Args: prefix (str): prefix to prepend to all buffer names. recurse (bool): if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module. Yields: (string, torch.Tensor): Tuple containing the name and buffer Example:: >>> for name, buf in self.named_buffers(): >>> if name in ['running_var']: >>> print(buf.size()) Parameters prefix ( str ) \u2013 defaults to `` recurse ( bool ) \u2013 defaults to True named_children Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple containing a name and child module Example:: >>> for name, module in model.named_children(): >>> if name in ['conv4', 'conv5']: >>> print(module) named_modules Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Yields: (string, Module): Tuple of name and module Note: Duplicate modules are returned only once. In the following example, l will be returned only once. Example:: >>> l = nn.Linear(2, 2) >>> net = nn.Sequential(l, l) >>> for idx, m in enumerate(net.named_modules()): print(idx, '->', m) 0 -> ('', Sequential( (0): Linear(in_features=2, out_features=2, bias=True) (1): Linear(in_features=2, out_features=2, bias=True) )) 1 -> ('0', Linear(in_features=2, out_features=2, bias=True)) Parameters memo ( Union[Set[ForwardRef('Module')], NoneType] ) \u2013 defaults to None prefix ( str ) \u2013 defaults to `` named_parameters Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself. Args: prefix (str): prefix to prepend to all parameter names. recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: (string, Parameter): Tuple containing the name and parameter Example:: >>> for name, param in self.named_parameters(): >>> if name in ['bias']: >>> print(param.size()) Parameters prefix ( str ) \u2013 defaults to `` recurse ( bool ) \u2013 defaults to True negative_encoding parameters Returns an iterator over module parameters. This is typically passed to an optimizer. Args: recurse (bool): if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module. Yields: Parameter: module parameter Example:: >>> for param in model.parameters(): >>> print(type(param), param.size()) (20L,) (20L, 1L, 5L, 5L) Parameters recurse ( bool ) \u2013 defaults to True register_backward_hook Registers a backward hook on the module. This function is deprecated in favor of :meth: nn.Module.register_full_backward_hook and the behavior of this function will change in future versions. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Parameters hook ( Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]] ) register_buffer Adds a buffer to the module. This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm's running_mean is not a parameter, but is part of the module's state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting :attr: persistent to False . The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module's :attr: state_dict . Buffers can be accessed as attributes using given names. Args: name (string): name of the buffer. The buffer can be accessed from this module using the given name tensor (Tensor): buffer to be registered. persistent (bool): whether the buffer is part of this module's :attr: state_dict . Example:: >>> self.register_buffer('running_mean', torch.zeros(num_features)) Parameters name ( str ) tensor ( Union[torch.Tensor, NoneType] ) persistent ( bool ) \u2013 defaults to True register_forward_hook Registers a forward hook on the module. The hook will be called every time after :func: forward has computed an output. It should have the following signature:: hook(module, input, output) -> None or modified output The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after :func: forward is called. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Parameters hook ( Callable[..., NoneType] ) register_forward_pre_hook Registers a forward pre-hook on the module. The hook will be called every time before :func: forward is invoked. It should have the following signature:: hook(module, input) -> None or modified input The input contains only the positional arguments given to the module. Keyword arguments won't be passed to the hooks and only to the forward . The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple). Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Parameters hook ( Callable[..., NoneType] ) register_full_backward_hook Registers a backward hook on the module. The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:: hook(module, grad_input, grad_output) -> tuple(Tensor) or None The :attr: grad_input and :attr: grad_output are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of :attr: grad_input in subsequent computations. :attr: grad_input will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in :attr: grad_input and :attr: grad_output will be None for all non-Tensor arguments. .. warning :: Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error. Returns: :class: torch.utils.hooks.RemovableHandle : a handle that can be used to remove the added hook by calling handle.remove() Parameters hook ( Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]] ) register_parameter Adds a parameter to the module. The parameter can be accessed as an attribute using given name. Args: name (string): name of the parameter. The parameter can be accessed from this module using the given name param (Parameter): parameter to be added to the module. Parameters name ( str ) param ( Union[torch.nn.parameter.Parameter, NoneType] ) requires_grad_ Change if autograd should record operations on parameters in this module. This method sets the parameters' :attr: requires_grad attributes in-place. This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training). Args: requires_grad (bool): whether autograd should record operations on parameters in this module. Default: True . Returns: Module: self Parameters requires_grad ( bool ) \u2013 defaults to True save share_memory state_dict Returns a dictionary containing a whole state of the module. Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Returns: dict: a dictionary containing a whole state of the module Example:: >>> module.state_dict().keys() ['bias', 'weight'] Parameters destination \u2013 defaults to None prefix \u2013 defaults to `` keep_vars \u2013 defaults to False tail_batch Used to get faster when computing scores for negative samples. Parameters sample negative_sample to Moves and/or casts the parameters and buffers. This can be called as .. function:: to(device=None, dtype=None, non_blocking=False) .. function:: to(dtype, non_blocking=False) .. function:: to(tensor, non_blocking=False) .. function:: to(memory_format=torch.channels_last) Its signature is similar to :meth: torch.Tensor.to , but only accepts floating point or complex :attr: dtype s. In addition, this method will only cast the floating point or complex parameters and buffers to :attr: dtype (if given). The integral parameters and buffers will be moved :attr: device , if that is given, but with dtypes unchanged. When :attr: non_blocking is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices. See below for examples. .. note:: This method modifies the module in-place. Args: device (:class: torch.device ): the desired device of the parameters and buffers in this module dtype (:class: torch.dtype ): the desired floating point or complex dtype of the parameters and buffers in this module tensor (torch.Tensor): Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module memory_format (:class: torch.memory_format ): the desired memory format for 4D parameters and buffers in this module (keyword only argument) Returns: Module: self Examples:: >>> linear = nn.Linear(2, 2) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]]) >>> linear.to(torch.double) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1913, -0.3420], [-0.5113, -0.2325]], dtype=torch.float64) >>> gpu1 = torch.device(\"cuda:1\") >>> linear.to(gpu1, dtype=torch.half, non_blocking=True) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1') >>> cpu = torch.device(\"cpu\") >>> linear.to(cpu) Linear(in_features=2, out_features=2, bias=True) >>> linear.weight Parameter containing: tensor([[ 0.1914, -0.3420], [-0.5112, -0.2324]], dtype=torch.float16) >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble) >>> linear.weight Parameter containing: tensor([[ 0.3741+0.j, 0.2382+0.j], [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128) >>> linear(torch.ones(3, 2, dtype=torch.cdouble)) tensor([[0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j], [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128) Parameters args kwargs train Sets the module in training mode. This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. :class: Dropout , :class: BatchNorm , etc. Args: mode (bool): whether to set training mode ( True ) or evaluation mode ( False ). Default: True . Returns: Module: self Parameters mode ( bool ) \u2013 defaults to True type Casts all parameters and buffers to :attr: dst_type . Args: dst_type (type or string): the desired type Returns: Module: self Parameters dst_type ( Union[torch.dtype, str] ) xpu Moves all model parameters and buffers to the XPU. This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized. Arguments: device (int, optional): if specified, all parameters will be copied to that device Returns: Module: self Parameters device ( Union[int, torch.device, NoneType] ) \u2013 defaults to None zero_grad Sets gradients of all model parameters to zero. See similar function under :class: torch.optim.Optimizer for more context. Args: set_to_none (bool): instead of setting to zero, set the grads to None. See :meth: torch.optim.Optimizer.zero_grad for details. Parameters set_to_none ( bool ) \u2013 defaults to False","title":"Methods"},{"location":"api/sampling/NegativeSampling/","text":"NegativeSampling \u00b6 Generate negative sample to train models. Parameters \u00b6 size train_triples entities relations seed \u2013 defaults to 42 Examples \u00b6 >>> from ckb import datasets >>> from ckb import sampling >>> import torch >>> _ = torch . manual_seed ( 42 ) >>> train = [ ... ( \"Le stratege\" , \"is_available\" , \"Netflix\" ), ... ( \"The Imitation Game\" , \"is_available\" , \"Netflix\" ), ... ( \"Star Wars\" , \"is_available\" , \"Disney\" ), ... ( \"James Bond\" , \"is_available\" , \"Amazon\" ), ... ] >>> dataset = datasets . Dataset ( ... train = train , ... batch_size = 2 , ... seed = 42 , ... shuffle = False , ... ) >>> negative_sampling = sampling . NegativeSampling ( ... size = 5 , ... train_triples = dataset . train , ... entities = dataset . entities , ... relations = dataset . relations , ... seed = 42 , ... ) >>> sample = torch . tensor ([[ 0 , 0 , 4 ], [ 1 , 0 , 4 ]]) >>> negative_sample = negative_sampling . generate ( sample , mode = 'tail-batch' ) >>> negative_sample tensor ([[ 6 , 3 , 6 , 2 , 6 ], [ 6 , 3 , 6 , 2 , 6 ]]) >>> negative_sample = negative_sampling . generate ( sample , mode = 'head-batch' ) >>> negative_sample tensor ([[ 6 , 2 , 2 , 4 , 3 ], [ 6 , 2 , 2 , 4 , 3 ]]) Methods \u00b6 generate Generate negative samples from a head, relation tail If the mode is set to head-batch, this method will generate a tensor of fake heads. If the mode is set to tail-batch, this method will generate a tensor of fake tails. Parameters sample mode get_true_head_and_tail Build a dictionary to filter out existing triples from fakes ones. triples References \u00b6 RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space \u21a9","title":"NegativeSampling"},{"location":"api/sampling/NegativeSampling/#negativesampling","text":"Generate negative sample to train models.","title":"NegativeSampling"},{"location":"api/sampling/NegativeSampling/#parameters","text":"size train_triples entities relations seed \u2013 defaults to 42","title":"Parameters"},{"location":"api/sampling/NegativeSampling/#examples","text":">>> from ckb import datasets >>> from ckb import sampling >>> import torch >>> _ = torch . manual_seed ( 42 ) >>> train = [ ... ( \"Le stratege\" , \"is_available\" , \"Netflix\" ), ... ( \"The Imitation Game\" , \"is_available\" , \"Netflix\" ), ... ( \"Star Wars\" , \"is_available\" , \"Disney\" ), ... ( \"James Bond\" , \"is_available\" , \"Amazon\" ), ... ] >>> dataset = datasets . Dataset ( ... train = train , ... batch_size = 2 , ... seed = 42 , ... shuffle = False , ... ) >>> negative_sampling = sampling . NegativeSampling ( ... size = 5 , ... train_triples = dataset . train , ... entities = dataset . entities , ... relations = dataset . relations , ... seed = 42 , ... ) >>> sample = torch . tensor ([[ 0 , 0 , 4 ], [ 1 , 0 , 4 ]]) >>> negative_sample = negative_sampling . generate ( sample , mode = 'tail-batch' ) >>> negative_sample tensor ([[ 6 , 3 , 6 , 2 , 6 ], [ 6 , 3 , 6 , 2 , 6 ]]) >>> negative_sample = negative_sampling . generate ( sample , mode = 'head-batch' ) >>> negative_sample tensor ([[ 6 , 2 , 2 , 4 , 3 ], [ 6 , 2 , 2 , 4 , 3 ]])","title":"Examples"},{"location":"api/sampling/NegativeSampling/#methods","text":"generate Generate negative samples from a head, relation tail If the mode is set to head-batch, this method will generate a tensor of fake heads. If the mode is set to tail-batch, this method will generate a tensor of fake tails. Parameters sample mode get_true_head_and_tail Build a dictionary to filter out existing triples from fakes ones. triples","title":"Methods"},{"location":"api/sampling/NegativeSampling/#references","text":"RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space \u21a9","title":"References"},{"location":"api/scoring/ComplEx/","text":"ComplEx \u00b6 ComplEx scoring function. Attributes \u00b6 name Examples \u00b6 >>> from ckb import models >>> from ckb import datasets >>> from ckb import scoring >>> import torch >>> _ = torch . manual_seed ( 42 ) >>> dataset = datasets . Semanlink ( 1 ) >>> model = models . DistillBert ( ... entities = dataset . entities , ... relations = dataset . relations , ... gamma = 9 , ... device = 'cpu' , ... scoring = scoring . ComplEx (), ... ) >>> sample = torch . tensor ([[ 0 , 0 , 0 ], [ 2 , 2 , 2 ]]) >>> model ( sample ) tensor ([[ 0.8402 ], [ 0.4317 ]], grad_fn =< ViewBackward > ) >>> sample = torch . tensor ([[ 0 , 0 , 1 ], [ 2 , 2 , 1 ]]) >>> model ( sample ) tensor ([[ 0.5372 ], [ 0.1728 ]], grad_fn =< ViewBackward > ) >>> sample = torch . tensor ([[ 1 , 0 , 0 ], [ 1 , 2 , 2 ]]) >>> model ( sample ) tensor ([[ 0.5762 ], [ 0.3085 ]], grad_fn =< ViewBackward > ) >>> sample = torch . tensor ([[ 0 , 0 , 0 ], [ 2 , 2 , 2 ]]) >>> negative_sample = torch . tensor ([[ 1 , 0 ], [ 1 , 2 ]]) >>> model ( sample , negative_sample , mode = 'head-batch' ) tensor ([[ 0.5762 , 0.8402 ], [ 0.3085 , 0.4317 ]], grad_fn =< ViewBackward > ) >>> model ( sample , negative_sample , mode = 'tail-batch' ) tensor ([[ 0.5372 , 0.8402 ], [ 0.1728 , 0.4317 ]], grad_fn =< ViewBackward > ) Methods \u00b6 call Compute the score of given facts (heads, relations, tails). Parameters head relation tail mode kwargs","title":"ComplEx"},{"location":"api/scoring/ComplEx/#complex","text":"ComplEx scoring function.","title":"ComplEx"},{"location":"api/scoring/ComplEx/#attributes","text":"name","title":"Attributes"},{"location":"api/scoring/ComplEx/#examples","text":">>> from ckb import models >>> from ckb import datasets >>> from ckb import scoring >>> import torch >>> _ = torch . manual_seed ( 42 ) >>> dataset = datasets . Semanlink ( 1 ) >>> model = models . DistillBert ( ... entities = dataset . entities , ... relations = dataset . relations , ... gamma = 9 , ... device = 'cpu' , ... scoring = scoring . ComplEx (), ... ) >>> sample = torch . tensor ([[ 0 , 0 , 0 ], [ 2 , 2 , 2 ]]) >>> model ( sample ) tensor ([[ 0.8402 ], [ 0.4317 ]], grad_fn =< ViewBackward > ) >>> sample = torch . tensor ([[ 0 , 0 , 1 ], [ 2 , 2 , 1 ]]) >>> model ( sample ) tensor ([[ 0.5372 ], [ 0.1728 ]], grad_fn =< ViewBackward > ) >>> sample = torch . tensor ([[ 1 , 0 , 0 ], [ 1 , 2 , 2 ]]) >>> model ( sample ) tensor ([[ 0.5762 ], [ 0.3085 ]], grad_fn =< ViewBackward > ) >>> sample = torch . tensor ([[ 0 , 0 , 0 ], [ 2 , 2 , 2 ]]) >>> negative_sample = torch . tensor ([[ 1 , 0 ], [ 1 , 2 ]]) >>> model ( sample , negative_sample , mode = 'head-batch' ) tensor ([[ 0.5762 , 0.8402 ], [ 0.3085 , 0.4317 ]], grad_fn =< ViewBackward > ) >>> model ( sample , negative_sample , mode = 'tail-batch' ) tensor ([[ 0.5372 , 0.8402 ], [ 0.1728 , 0.4317 ]], grad_fn =< ViewBackward > )","title":"Examples"},{"location":"api/scoring/ComplEx/#methods","text":"call Compute the score of given facts (heads, relations, tails). Parameters head relation tail mode kwargs","title":"Methods"},{"location":"api/scoring/DistMult/","text":"DistMult \u00b6 DistMult scoring function. Attributes \u00b6 name Examples \u00b6 >>> from ckb import models >>> from ckb import datasets >>> from ckb import scoring >>> import torch >>> _ = torch . manual_seed ( 42 ) >>> dataset = datasets . Semanlink ( 1 ) >>> model = models . DistillBert ( ... entities = dataset . entities , ... relations = dataset . relations , ... gamma = 9 , ... device = 'cpu' , ... scoring = scoring . DistMult (), ... ) >>> sample = torch . tensor ([[ 0 , 0 , 0 ], [ 2 , 2 , 2 ]]) >>> model ( sample ) tensor ([[ - 0.3350 ], [ - 0.8084 ]], grad_fn =< ViewBackward > ) >>> sample = torch . tensor ([[ 0 , 0 , 1 ], [ 2 , 2 , 1 ]]) >>> model ( sample ) tensor ([[ - 0.3135 ], [ - 0.5852 ]], grad_fn =< ViewBackward > ) >>> sample = torch . tensor ([[ 1 , 0 , 0 ], [ 1 , 2 , 2 ]]) >>> model ( sample ) tensor ([[ - 0.3135 ], [ - 0.5852 ]], grad_fn =< ViewBackward > ) >>> sample = torch . tensor ([[ 0 , 0 , 0 ], [ 2 , 2 , 2 ]]) >>> negative_sample = torch . tensor ([[ 1 , 0 ], [ 1 , 2 ]]) >>> model ( sample , negative_sample , mode = 'head-batch' ) tensor ([[ - 0.3135 , - 0.3350 ], [ - 0.5852 , - 0.8084 ]], grad_fn =< ViewBackward > ) >>> model ( sample , negative_sample , mode = 'tail-batch' ) tensor ([[ - 0.3135 , - 0.3350 ], [ - 0.5852 , - 0.8084 ]], grad_fn =< ViewBackward > ) Methods \u00b6 call Compute the score of given facts (heads, relations, tails). Parameters head relation tail mode kwargs","title":"DistMult"},{"location":"api/scoring/DistMult/#distmult","text":"DistMult scoring function.","title":"DistMult"},{"location":"api/scoring/DistMult/#attributes","text":"name","title":"Attributes"},{"location":"api/scoring/DistMult/#examples","text":">>> from ckb import models >>> from ckb import datasets >>> from ckb import scoring >>> import torch >>> _ = torch . manual_seed ( 42 ) >>> dataset = datasets . Semanlink ( 1 ) >>> model = models . DistillBert ( ... entities = dataset . entities , ... relations = dataset . relations , ... gamma = 9 , ... device = 'cpu' , ... scoring = scoring . DistMult (), ... ) >>> sample = torch . tensor ([[ 0 , 0 , 0 ], [ 2 , 2 , 2 ]]) >>> model ( sample ) tensor ([[ - 0.3350 ], [ - 0.8084 ]], grad_fn =< ViewBackward > ) >>> sample = torch . tensor ([[ 0 , 0 , 1 ], [ 2 , 2 , 1 ]]) >>> model ( sample ) tensor ([[ - 0.3135 ], [ - 0.5852 ]], grad_fn =< ViewBackward > ) >>> sample = torch . tensor ([[ 1 , 0 , 0 ], [ 1 , 2 , 2 ]]) >>> model ( sample ) tensor ([[ - 0.3135 ], [ - 0.5852 ]], grad_fn =< ViewBackward > ) >>> sample = torch . tensor ([[ 0 , 0 , 0 ], [ 2 , 2 , 2 ]]) >>> negative_sample = torch . tensor ([[ 1 , 0 ], [ 1 , 2 ]]) >>> model ( sample , negative_sample , mode = 'head-batch' ) tensor ([[ - 0.3135 , - 0.3350 ], [ - 0.5852 , - 0.8084 ]], grad_fn =< ViewBackward > ) >>> model ( sample , negative_sample , mode = 'tail-batch' ) tensor ([[ - 0.3135 , - 0.3350 ], [ - 0.5852 , - 0.8084 ]], grad_fn =< ViewBackward > )","title":"Examples"},{"location":"api/scoring/DistMult/#methods","text":"call Compute the score of given facts (heads, relations, tails). Parameters head relation tail mode kwargs","title":"Methods"},{"location":"api/scoring/RotatE/","text":"RotatE \u00b6 RotatE scoring function. Attributes \u00b6 name Examples \u00b6 >>> from ckb import models >>> from ckb import datasets >>> from ckb import scoring >>> import torch >>> _ = torch . manual_seed ( 42 ) >>> dataset = datasets . Semanlink ( 1 ) >>> model = models . DistillBert ( ... entities = dataset . entities , ... relations = dataset . relations , ... gamma = 9 , ... device = 'cpu' , ... scoring = scoring . RotatE (), ... ) >>> sample = torch . tensor ([[ 0 , 0 , 1 ], [ 2 , 2 , 1 ]]) >>> model ( sample ) tensor ([[ - 203.6809 ], [ - 191.3758 ]], grad_fn =< ViewBackward > ) >>> sample = torch . tensor ([[ 1 , 0 , 0 ], [ 1 , 2 , 2 ]]) >>> model ( sample ) tensor ([[ - 204.0743 ], [ - 192.8306 ]], grad_fn =< ViewBackward > ) Methods \u00b6 call Compute the score of given facts (heads, relations, tails). Parameters head relation tail gamma embedding_range mode kwargs","title":"RotatE"},{"location":"api/scoring/RotatE/#rotate","text":"RotatE scoring function.","title":"RotatE"},{"location":"api/scoring/RotatE/#attributes","text":"name","title":"Attributes"},{"location":"api/scoring/RotatE/#examples","text":">>> from ckb import models >>> from ckb import datasets >>> from ckb import scoring >>> import torch >>> _ = torch . manual_seed ( 42 ) >>> dataset = datasets . Semanlink ( 1 ) >>> model = models . DistillBert ( ... entities = dataset . entities , ... relations = dataset . relations , ... gamma = 9 , ... device = 'cpu' , ... scoring = scoring . RotatE (), ... ) >>> sample = torch . tensor ([[ 0 , 0 , 1 ], [ 2 , 2 , 1 ]]) >>> model ( sample ) tensor ([[ - 203.6809 ], [ - 191.3758 ]], grad_fn =< ViewBackward > ) >>> sample = torch . tensor ([[ 1 , 0 , 0 ], [ 1 , 2 , 2 ]]) >>> model ( sample ) tensor ([[ - 204.0743 ], [ - 192.8306 ]], grad_fn =< ViewBackward > )","title":"Examples"},{"location":"api/scoring/RotatE/#methods","text":"call Compute the score of given facts (heads, relations, tails). Parameters head relation tail gamma embedding_range mode kwargs","title":"Methods"},{"location":"api/scoring/Scoring/","text":"Scoring \u00b6 Attributes \u00b6 name","title":"Scoring"},{"location":"api/scoring/Scoring/#scoring","text":"","title":"Scoring"},{"location":"api/scoring/Scoring/#attributes","text":"name","title":"Attributes"},{"location":"api/scoring/TransE/","text":"TransE \u00b6 TransE scoring function. Attributes \u00b6 name Examples \u00b6 >>> from ckb import scoring >>> scoring . TransE () TransE scoring Methods \u00b6 call Compute the score of given facts (heads, relations, tails). Parameters head relation tail gamma mode kwargs","title":"TransE"},{"location":"api/scoring/TransE/#transe","text":"TransE scoring function.","title":"TransE"},{"location":"api/scoring/TransE/#attributes","text":"name","title":"Attributes"},{"location":"api/scoring/TransE/#examples","text":">>> from ckb import scoring >>> scoring . TransE () TransE scoring","title":"Examples"},{"location":"api/scoring/TransE/#methods","text":"call Compute the score of given facts (heads, relations, tails). Parameters head relation tail gamma mode kwargs","title":"Methods"},{"location":"api/scoring/pRotatE/","text":"pRotatE \u00b6 pRotatE scoring function. Attributes \u00b6 name Examples \u00b6 >>> from ckb import models >>> from ckb import datasets >>> from ckb import scoring >>> import torch >>> _ = torch . manual_seed ( 42 ) >>> dataset = datasets . Semanlink ( 1 ) >>> model = models . DistillBert ( ... entities = dataset . entities , ... relations = dataset . relations , ... gamma = 9 , ... device = 'cpu' , ... scoring = scoring . pRotatE (), ... ) >>> sample = torch . tensor ([[ 0 , 0 , 0 ], [ 2 , 2 , 2 ]]) >>> model ( sample ) tensor ([[ 5.4199 ], [ 5.4798 ]], grad_fn =< ViewBackward > ) >>> sample = torch . tensor ([[ 0 , 0 , 1 ], [ 2 , 2 , 1 ]]) >>> model ( sample ) tensor ([[ 5.4521 ], [ 5.5248 ]], grad_fn =< ViewBackward > ) >>> sample = torch . tensor ([[ 1 , 0 , 0 ], [ 1 , 2 , 2 ]]) >>> model ( sample ) tensor ([[ 5.4962 ], [ 5.5059 ]], grad_fn =< ViewBackward > ) >>> sample = torch . tensor ([[ 0 , 0 , 0 ], [ 2 , 2 , 2 ]]) >>> negative_sample = torch . tensor ([[ 1 , 0 ], [ 1 , 2 ]]) >>> model ( sample , negative_sample , mode = 'head-batch' ) tensor ([[ 5.4962 , 5.4199 ], [ 5.5059 , 5.4798 ]], grad_fn =< ViewBackward > ) >>> model ( sample , negative_sample , mode = 'tail-batch' ) tensor ([[ 5.4521 , 5.4199 ], [ 5.5248 , 5.4798 ]], grad_fn =< ViewBackward > ) Methods \u00b6 call Compute the score of given facts (heads, relations, tails). Parameters head relation tail gamma embedding_range modulus mode kwargs","title":"pRotatE"},{"location":"api/scoring/pRotatE/#protate","text":"pRotatE scoring function.","title":"pRotatE"},{"location":"api/scoring/pRotatE/#attributes","text":"name","title":"Attributes"},{"location":"api/scoring/pRotatE/#examples","text":">>> from ckb import models >>> from ckb import datasets >>> from ckb import scoring >>> import torch >>> _ = torch . manual_seed ( 42 ) >>> dataset = datasets . Semanlink ( 1 ) >>> model = models . DistillBert ( ... entities = dataset . entities , ... relations = dataset . relations , ... gamma = 9 , ... device = 'cpu' , ... scoring = scoring . pRotatE (), ... ) >>> sample = torch . tensor ([[ 0 , 0 , 0 ], [ 2 , 2 , 2 ]]) >>> model ( sample ) tensor ([[ 5.4199 ], [ 5.4798 ]], grad_fn =< ViewBackward > ) >>> sample = torch . tensor ([[ 0 , 0 , 1 ], [ 2 , 2 , 1 ]]) >>> model ( sample ) tensor ([[ 5.4521 ], [ 5.5248 ]], grad_fn =< ViewBackward > ) >>> sample = torch . tensor ([[ 1 , 0 , 0 ], [ 1 , 2 , 2 ]]) >>> model ( sample ) tensor ([[ 5.4962 ], [ 5.5059 ]], grad_fn =< ViewBackward > ) >>> sample = torch . tensor ([[ 0 , 0 , 0 ], [ 2 , 2 , 2 ]]) >>> negative_sample = torch . tensor ([[ 1 , 0 ], [ 1 , 2 ]]) >>> model ( sample , negative_sample , mode = 'head-batch' ) tensor ([[ 5.4962 , 5.4199 ], [ 5.5059 , 5.4798 ]], grad_fn =< ViewBackward > ) >>> model ( sample , negative_sample , mode = 'tail-batch' ) tensor ([[ 5.4521 , 5.4199 ], [ 5.5248 , 5.4798 ]], grad_fn =< ViewBackward > )","title":"Examples"},{"location":"api/scoring/pRotatE/#methods","text":"call Compute the score of given facts (heads, relations, tails). Parameters head relation tail gamma embedding_range modulus mode kwargs","title":"Methods"},{"location":"api/utils/read-csv/","text":"read_csv \u00b6 Read a csv files of triplets and convert it to list of triplets. Parameters \u00b6 path sep header \u2013 defaults to None","title":"read_csv"},{"location":"api/utils/read-csv/#read_csv","text":"Read a csv files of triplets and convert it to list of triplets.","title":"read_csv"},{"location":"api/utils/read-csv/#parameters","text":"path sep header \u2013 defaults to None","title":"Parameters"}]}